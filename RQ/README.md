# Research Questions


Our experimental evaluation answers the research questions below.

## RQ1: Do we find more faults than Random Testing with the same testing budget?

*In this research question we want to study the effectiveness of STARLA in finding more faults than Random Testing when we consider the same testing budget, measured as the number of executed episodes.*

To do so, we consider two practical testing scenarios:
In both scenarios training episodes of the RL agent are given.
### Scenario1: Randomly executed episodes are available or inexpensive: 
In this scenario, we can consider that episodes of random executions of the agent are available. 
One example is when the agent is tested to some extent. However, before final deployment, we want further test the agent using STARLA. 
Another situation is when the RL agent is trained and tested using both a simulator and hardware in the loop [4].

In this situation, an agent is trained and tested on a simulator in order to have a ”warm-start” learning on real hardware [4]. Since STARLA produces episodes with a high fault probability, we can use it to test the agent when executed on real hardware to further assess the reliability of the agent. In such situation, STARLA uses episodes that are generated with a simulator and executes the newly generated episodes on the hardware.

More precisely, the total testing budget in this scenario is equal to:

Mutated episodes that have been executed during the search + Faulty episodes generated by STARLA (executed after the search)


### Senario2: Randomly executed episodes are generated with STARLA and should be accounted for in the testing budget: 
In the second scenario, we assume that the agent is trained but not tested so far and we want to test the agent using STARLA. Therefore, we need to use part of our testing budget for random executions, to generate the required episodes. 

More precisely, the total testing budget in this scenario is equal to:

The number of episodes in the initial population (generated through random executions of the agent) + Mutated episodes that have been executed during the search + Faulty episodes generated by STARLA (executed after the search)

<p align="center" width="100%">
    <img width="60%" src="https://user-images.githubusercontent.com/23516995/171270072-a3b45f92-a13c-44a6-8f62-542f7b0aaba1.png"> 
</p>


**Answer:** For both scenarios, we find significantly more functional faults with STARLA than with Random Testing using the same testing budget. 


## RQ2: Can we rely on ML models to predict faulty episodes?

*In this research question we investigate the accuracy of ML classifiers in predicting faulty episodes of the RL agent.*

We use Random Forest to predict the probabilities of reward and functional faults in a given episode.
To build our training dataset, we sampled episodes from both episodes generated through random executions of the agent and episodes from the training phase of the agent. Episodes are encoded based on the presence or absence of their abstract states. We have two different ML models, one for predicting the probability of a reward fault and the other one for predicting the probability of a functional fault. We considered 70 % of data for training and 30% for testing.

<p align="center" width="100%">
    <img width="50%" src="https://user-images.githubusercontent.com/23516995/212187951-5a9e0ceb-f209-45d7-88d0-f41df9da007e.png"> 
</p>



**Answer:** Using the mentioned ML classifier and feature representation, we can accurately classify the episodes of RL agents as having functional faults, reward faults, or no fault at all.



## RQ3. Can we learn accurate rules to characterize the faulty episodes of RL agents?

*Here, we investigate the learning of interpretable rules that characterize faulty episodes to understand the conditions under which the RL agent can be expected to fail.*


For this reason, we need to rely on an interpretable ML model, in this case, a Decision Tree model, to learn such rules.
We assess the accuracy of decision trees and therefore our ability to learn accurate rules based on the faulty episodes that we identify with STARLA. 
In practice, engineers will need to use such an approach to assess the safety of using an RL agent and understand the reasons of faults.
In this part, we assess the accuracy of trained models that extract the rules of functional and reward faults based on k-fold cross-validation.



<p align="center" width="100%">
    <img width="50%" src="https://user-images.githubusercontent.com/23516995/169616496-bebacddf-cb97-4ab3-bcf9-cfb8b654a4ee.png"> 
</p>


Such highly accurate rules can help developers understand the conditions under which the agent fails. One can analyze, the concrete states that correspond to abstract states leading to faults to extract real-world conditions of failure. 
For example, we extracted the following faulty rule $Not(S^\phi_{5})$ and $S^\phi_{12}$ and $S^\phi_{23}$ from our decision tree. First we extract all faulty episodes following this rule. Then, we extract from these episodes all concrete states belonging to the abstract states with the condition of presence in **R1**, i.e., $S^\phi_{12}$ and $S^\phi_{23}$.
For abstract states $S^\phi_5$ where the rule states they should be absent, we extract the set of all corresponding concrete states from all episodes in the final dataset.
Finally, for each abstract state in the rule, we analyze the distribution of each characteristic of the corresponding concrete states (i.e., the position of the cart, the velocity, the angle of the pole and the angular velocity) to interpret the situations under which the agent fails. below you see the boxplots of the mentioned distributions.

<p align="center" width="100%">
    <img width="100%" src="https://user-images.githubusercontent.com/23516995/171912017-75548e5b-9151-42f1-afbc-ffafbd8d163e.png"> 
</p>



<p align="center" width="100%">
    <img width="100%" src="https://user-images.githubusercontent.com/23516995/171912194-b58beb6f-7cdd-402a-99d2-193b1c2f1664.png"> 
</p>


<p align="center" width="100%">
    <img width="100%" src="https://user-images.githubusercontent.com/23516995/171912131-db832638-ea55-4754-a297-a8ae05a98b64.png"> 
</p>


Moreover, we rely on the median values of the distribution of the states' characteristics to illustrate each abstract state and hence the failing conditions. 
We illustrate in the following figure an interpretation of such conditions.


<p align="center" width="100%">
    <img width="50%" src="https://user-images.githubusercontent.com/23516995/171913026-a1713863-4ac8-46d9-b930-6a20b7ba1a53.png"> 
</p>

Each cart represents one abstract state. The Gray cart depicts the state of the system in abstract state $S^\phi_5$, which should be absent in the episode. The black carts represent the presence of abstract states $S^\phi_{12}$ and $S^\phi_{23}$, respectively. Having both states of the cart shown in the right as and not having the state at the left indicate a fault.

We realized that the presence of abstract states $S^\phi_{12}$ and $S^\phi_{23}$ represent situations where the cart is close to the right border of the track and the angle of the pole is towards the right. To compensate for the large angle of the pole, as you can see in the figure, the agent has no choice but to push the cart to the right, which results in a fault because of passing the border. Moreover, abstract state $S^\phi_{5}$ represents a situation where the angle of the pole is not large, and the position of the cart is toward the right but not close to the border. In such situation, the agent will be able to control the pole in the remaining area and keep the pole upright without crossing the border, which justifies why such abstract state should be absent in faulty episodes that satisfy rule **R1**.

**Answer:** By using STARLA and interpretable ML models, such as Decision Trees, we can accurately learn rules that characterize the faulty episodes of RL agents.

