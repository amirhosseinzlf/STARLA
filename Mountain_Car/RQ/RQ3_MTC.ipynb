{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmEt6P_dGJ2x"
      },
      "source": [
        "#Initiate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4STng9lGJ2x"
      },
      "source": [
        "##Requirements "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrGQG1_kGJ2x",
        "outputId": "db8f00ed-fb73-4406-e05d-544808847986"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for external in metadata.entry_points().get(self.group, []):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
            "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
          ]
        }
      ],
      "source": [
        "# install the following libraries\n",
        "# !pip install stable-baselines==2.10.2\n",
        "# !pip install pymoo==0.4.2.2\n",
        "# # !pip install pycaret\n",
        "# # !pip install tensorflow==1.14\n",
        "# # %tensorflow_version 1.x\n",
        "# !pip install importlib-metadata==4.13.0\n",
        "# # !pip install tensorflow==1.15.2\n",
        "# !pip install gym==0.21.0\n",
        "\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines import DQN\n",
        "from copy import deepcopy\n",
        "import math\n",
        "from gym.spaces import Discrete, Dict, Box\n",
        "from gym import spaces\n",
        "from random import seed\n",
        "import random \n",
        "from gym import Env\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "import stable_baselines\n",
        "import sklearn\n",
        "import numpy\n",
        "from sklearn import tree , svm \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB , CategoricalNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import product\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import KFold , RepeatedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import impute\n",
        "import statistics\n",
        "from scipy import stats\n",
        "from copy import deepcopy\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from math import ceil\n",
        "import copy\n",
        "import sys\n",
        "from sklearn.metrics import jaccard_score\n",
        "import time\n",
        "import multiprocessing\n",
        "from pymoo.algorithms.nsga2 import calc_crowding_distance\n",
        "sys.path.append('lib/')\n",
        "import subprocess\n",
        "import logging\n",
        "from sklearn.utils import shuffle\n",
        "import csv\n",
        "from csv import reader\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeYrtITtGJ2y",
        "outputId": "b46830a0-7ae3-4fa9-d57d-bd27f8934f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from ipython-autotime) (7.9.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->ipython-autotime) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1 jedi-0.18.2\n",
            "time: 595 µs (started: 2022-12-06 18:27:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4NBDxnDGJ2y"
      },
      "source": [
        "##RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FvJl_DfJGJ2y"
      },
      "outputs": [],
      "source": [
        "\n",
        "class StoreAndTerminateWrapper(gym.Wrapper):\n",
        "  ''''\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  :param max_steps: (int) Max number of steps per episode\n",
        "  '''\n",
        "  def __init__(self, env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(StoreAndTerminateWrapper, self).__init__(env)\n",
        "    self.max_steps = 200\n",
        "    # Counter of steps per episode\n",
        "    self.current_step = 0\n",
        "    self.mem = []\n",
        "    self.TotalReward = 0.0 \n",
        "    self.env = env\n",
        "    self.first_state = 0\n",
        "    self.first_obs = 0\n",
        "    self.prev_obs = 0 \n",
        "    self.states_list = []\n",
        "    self.info = {}\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    # Reset the counter\n",
        "    self.current_step = 0\n",
        "    obs =self.env.reset()\n",
        "    self.TotalReward = 0.0\n",
        "    self.first_obs = obs\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    In this function we store the initial state as well as the memory of the agent\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    if self.current_step == 0: #store initial state\n",
        "      self.prev_obs = self.first_obs\n",
        "      self.first_state = deepcopy(self.env)\n",
        "      self.states_list.append(self.first_state)\n",
        "    # print(\"t\",self.env.state[0],\"reward\",self.TotalReward)\n",
        "    # if self.env.state[0]==-1.2:\n",
        "    #   print(\"-1.2\")\n",
        "    #   obs = self.reset()\n",
        "    #   reward = -200\n",
        "    #   done = True\n",
        "    #   return obs, reward, done, False\n",
        "    self.current_step += 1\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    self.TotalReward += reward\n",
        "    self.mem.append(tuple((self.prev_obs,action)))\n",
        "    self.prev_obs = obs\n",
        "    if self.current_step >= self.max_steps:\n",
        "      done = True\n",
        "      # Update the info dict to signal that the limit was exceeded\n",
        "    if obs[0]<=-1.2:\n",
        "      done = True\n",
        "      reward = -201 - self.TotalReward\n",
        "      self.TotalReward =-200\n",
        "      # print(\"fff\",reward)\n",
        "    if done:\n",
        "      self.mem.append(tuple(('done',self.TotalReward)))\n",
        "    self.info['mem'] = self.mem\n",
        "    self.info['state'] = self.states_list\n",
        "    # self.mem.append(tuple(obs,action))\n",
        "    return obs, reward, done, info\n",
        "\n",
        "  def set_state(self, state):\n",
        "    \"\"\"\n",
        "    :param state: initial state of the episode\n",
        "    :return: environment is updated and observations is returned\n",
        "    \"\"\"\n",
        "    self.env = deepcopy(state)\n",
        "    obs = np.array(list(self.env.unwrapped.state))\n",
        "    self.current_step = 0\n",
        "    self.TotalReward = 0.0\n",
        "    self.first_obs = obs\n",
        "    return obs\n",
        "\n",
        "def proportional_sampling_whitout_replacement(index , size):\n",
        "  s=0\n",
        "  s = sum(np.array(index))\n",
        "  p = [ind/s for ind in index]\n",
        "  samples = np.random.choice(index,size=size,replace=False,p=p)\n",
        "  return samples\n",
        "\n",
        "\n",
        "def population_sample(episodes , ind,  pop_size , threshold, functional_fault_size, reward_fault_size):\n",
        "  \"\"\"\n",
        "  This function is meant to sample episodes from training after that you need to add test episodes using random_test \n",
        "  Set the parameters as you want but be careful the input episodes for this function is the memory of the agent and each step has seperate index \n",
        "  this function returns the final steps of the selected function then you need to extract that episodes from the input memore that is called 'episodes'\n",
        "  use the episodes extract function ... \n",
        "\n",
        "  samples n episodes from training n1 functinal faults and n2 reward faults \n",
        "  reward faults are episodes with reward bellow the thresthreshold \n",
        "  from random test samples M episodes m1 random episode and\n",
        "  m2 episodes with sudden reward change we dont have a sudden reward change in this example  \n",
        "  \"\"\"\n",
        "  epsilon = 0.1\n",
        "  index = []\n",
        "  functional_fault = []\n",
        "  reward_fault = []\n",
        "  start_states =[]\n",
        "  ind  = np.where(np.array(episodes)==('done',))\n",
        "  index= ind[0]\n",
        "  print(len(ind[0]),'episodes from training')\n",
        "  population=[]\n",
        "  for i in index:\n",
        "    _,r = episodes[i]\n",
        "    if abs(episodes[i-1][0][0])<(mtc_wrapped.low[0]+epsilon):\n",
        "      functional_fault.append(i)\n",
        "      print('function fault') \n",
        "    if r<threshold:\n",
        "      reward_fault.append(i)\n",
        "      print('reward fault')\n",
        "  if len(functional_fault)<functional_fault_size:\n",
        "    print('functional faults size is' ,len(functional_fault),' and its less than desired number' )\n",
        "    population += functional_fault\n",
        "    print('sampling more random episodes instead ...!')\n",
        "  if len(functional_fault)==functional_fault_size:\n",
        "    population += functional_fault\n",
        "  if len(functional_fault)>functional_fault_size:\n",
        "    # proportianl_sample_whitout_replacement()\n",
        "    sam1=proportional_sampling_whitout_replacement(functional_fault,functional_fault_size)\n",
        "    print(population)\n",
        "    print(\"ff\",len(functional_fault))\n",
        "    population += sam1\n",
        "  if len(reward_fault)<reward_fault_size:\n",
        "    print('reward faults size is' ,len(reward_fault),' and its less than desired number' )\n",
        "    population += reward_fault\n",
        "    print('sampling more random episodes instead ...!')\n",
        "  if len(reward_fault)==reward_fault_size:\n",
        "    population += reward_fault\n",
        "  if len(reward_fault)>reward_fault_size:\n",
        "    #proportional sampling\n",
        "    sam2 = proportional_sampling_whitout_replacement(reward_fault,reward_fault_size)\n",
        "    population += list(sam2)\n",
        "  r_size= pop_size-len(population)\n",
        "  # random_test(model,env,r_size)\n",
        "  print(\"RF\",len(reward_fault))\n",
        "  # population += reward_fault\n",
        "  return population , r_size\n",
        "\n",
        "\n",
        "def population_sample_generalaized(episodes ,  pop_size , threshold, functional_fault_size, reward_fault_size):\n",
        "  \"\"\"\n",
        "  This function is meant to sample episodes from training after that you need to add test episodes using random_test \n",
        "  Set the parameters as you want but be careful the input episodes for this function is the memory of the agent and each step has seperate index \n",
        "  this function returns the final steps of the selected function then you need to extract that episodes from the input memore that is called 'episodes'\n",
        "  use the episodes extract function ... \n",
        "\n",
        "  samples n episodes from training n1 functinal faults and n2 reward faults \n",
        "  reward faults are episodes with reward bellow the thresthreshold \n",
        "  from random test samples M episodes m1 random episode and\n",
        "  m2 episodes with sudden reward change we dont have a sudden reward change in this example  \n",
        "  \"\"\"\n",
        "  epsilon = 0.1\n",
        "  index = []\n",
        "  functional_fault = []\n",
        "  reward_fault = []\n",
        "  start_states =[]\n",
        "  ind  = np.where(np.array(episodes)==('done',))\n",
        "  index= ind[0]\n",
        "  print(len(ind[0]),'episodes from training')\n",
        "  population=[]\n",
        "  for i in index:\n",
        "    _,r = episodes[i]\n",
        "    if is_functional_fault_last_state(episodes[i-1],episodes[i]):\n",
        "      functional_fault.append(i)\n",
        "      # print('function fault') \n",
        "    if is_reward_fault_last_state(episodes[i-1],episodes[i]):\n",
        "      reward_fault.append(i)\n",
        "      # print('reward fault')\n",
        "  if len(functional_fault)<functional_fault_size:\n",
        "    print('functional faults size is' ,len(functional_fault),' and its less than desired number' )\n",
        "    population += functional_fault\n",
        "    print('sample more random episodes instead ...!')\n",
        "  if len(functional_fault)==functional_fault_size:\n",
        "    population += functional_fault\n",
        "  if len(functional_fault)>functional_fault_size:\n",
        "    # proportianl_sample_whitout_replacement()\n",
        "    sam1=proportional_sampling_whitout_replacement(functional_fault,functional_fault_size)\n",
        "    print(population)\n",
        "    print(\"ff\",len(functional_fault))\n",
        "    print(type(sam1))\n",
        "    population += list(sam1)\n",
        "  if len(reward_fault)<reward_fault_size:\n",
        "    print('reward faults size is' ,len(reward_fault),' and its less than desired number' )\n",
        "    population += reward_fault\n",
        "    print('sample more random episodes instead ...!')\n",
        "  if len(reward_fault)==reward_fault_size:\n",
        "    population += reward_fault\n",
        "  if len(reward_fault)>reward_fault_size:\n",
        "    #proportional sampling\n",
        "    sam2 = proportional_sampling_whitout_replacement(reward_fault,reward_fault_size)\n",
        "    population += list(sam2)\n",
        "  if len(set(population))<len(population):\n",
        "    print(\"Duplicated selection.\")#this can happen if in an environment both reward and functional faults can happen in one episode. \n",
        "  r_size= pop_size-len(population)\n",
        "  # random_test(model,env,r_size)\n",
        "  print(\"RF\",len(reward_fault))\n",
        "  # population += reward_fault\n",
        "  return population , r_size\n",
        "\n",
        "\n",
        "def episode_extract(sampled_index, episodes):\n",
        "  epis = []\n",
        "  for i in sampled_index:\n",
        "    l=abs(int(episodes[i][1]))\n",
        "    slice1 = episodes[(i-l):(i+1)]\n",
        "    epis.append(slice1)\n",
        "  return epis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def episode_extract_general(sampled_index, episodes):\n",
        "  epis = []\n",
        "  for i in sampled_index:\n",
        "    # print(episodes[i])\n",
        "    j = i-1\n",
        "    while not episodes[j][0] == 'done':\n",
        "      # print(episodes[j])\n",
        "      if j==0:\n",
        "        break\n",
        "      j-=1\n",
        "    slice1 = episodes[(j+1):(i+1)]\n",
        "    epis.append(slice1)\n",
        "    assert len(slice1)>0, 'Attempt to return Empty episode'\n",
        "  return epis\n",
        "\n",
        "\n",
        "def fitness_reward(episode):\n",
        "  \"\"\"\n",
        "  here the reward could be calculated as the lengh of the episode; Since the\n",
        "  reward of the cartpole is defined based on the number of steps without falling\n",
        "  last part of the episode contains the signal of ('done',reward)\n",
        "  \"\"\"\n",
        "  return len(episode)-1\n",
        "\n",
        "def fitness_reward_general(episode):\n",
        "  \"\"\"\n",
        "  here the reward could be calculated as the lengh of the episode; Since the\n",
        "  reward of the cartpole is defined based on the number of steps without falling\n",
        "  last part of the episode contains the signal of ('done',reward)\n",
        "  \"\"\"\n",
        "  return episode[-1][1]\n",
        "\n",
        "def fitness_confidence(episode, model, mode):\n",
        "  \"\"\"\n",
        "  confidence level is define as differences between the highest and\n",
        "  second highest action probabilities of selecting actions OR\n",
        "  the ratio between the highest and lowest/second highest action probability\n",
        "  :param `mode`: r for ration and m for differences \n",
        "  :param `model`: is the RL agent \n",
        "  :param `episode`: is the episode values or sequence from the rl \n",
        "  \"\"\"\n",
        "  cl = 0.0\n",
        "  for i in range(len(episode)):\n",
        "    if i==(len(episode)-1):\n",
        "        if episode[i][0]=='done':\n",
        "            return (cl/episode[i][1])\n",
        "        else:\n",
        "            assert False, \"last state is not done , reward\"\n",
        "    else:\n",
        "      prob=model.action_probability(episode[i][0])\n",
        "      high1=prob.argmax()\n",
        "      first = prob[high1]\n",
        "      temp = prob\n",
        "      temp[high1] = 0.0\n",
        "      high2= temp.argmax()\n",
        "      second = prob[high2]\n",
        "      if mode == 'r':\n",
        "        cl +=  (first/second)\n",
        "        #In the next version this will be updated to a normalized ratio to avoid having large values \n",
        "      if mode == 'm':\n",
        "        cl += (first - second) #To_Do: first - second / first +second this one is better \n",
        "  print(\"WARNING nothing returned\", episode )\n",
        "\n",
        "def fitness_confidence_general(episode, model, mode):\n",
        "  \"\"\"\n",
        "  confidence level is define as differences between the highest and\n",
        "  second highest action probabilities of selecting actions OR\n",
        "  the ratio between the highest and lowest/second highest action probability\n",
        "  :param `mode`: r for ration and m for differences \n",
        "  :param `model`: is the RL agent \n",
        "  :param `episode`: is the episode values or sequence from the rl \n",
        "  \"\"\"\n",
        "  cl = 0.0\n",
        "  for i in range(len(episode)):\n",
        "    if i==(len(episode)-1):\n",
        "        if episode[i][0]=='done':\n",
        "            return (cl/(len(episode)-1))\n",
        "        else:\n",
        "            assert False, \"last state is not done , reward\"\n",
        "    else:\n",
        "      prob=model.action_probability(episode[i][0])\n",
        "      high1=prob.argmax()\n",
        "      first = prob[high1]\n",
        "      temp = prob\n",
        "      temp[high1] = 0.0\n",
        "      high2= temp.argmax()\n",
        "      second = prob[high2]\n",
        "      if mode == 'r':\n",
        "        cl +=  (first/second)\n",
        "        #In the next version this will be updated to a normalized ratio to avoid having large values \n",
        "      if mode == 'm':\n",
        "        cl += (first - second) #To_Do: first - second / first +second this one is better \n",
        "  print(\"WARNING nothing returned\", episode )\n",
        "\n",
        "\n",
        "def fitness_reward_probability(ml, binary_episode):\n",
        "  \"\"\"\n",
        "  This function returns the third fitness funciton that is ment to guide the search toward\n",
        "  the episodes with a higher probability of a reward fault and as we have a minimizing \n",
        "  optimization funciton in MOSA we neeed to change this functionwe can either go with the\n",
        "  negation of the probability of the reward fault = 1-probability of the reward fault\n",
        "  that is equal to the probability of the bein a non-faulty episode\n",
        "  :param `ml`: RF_FF_1rep for functional fault\n",
        "  :param `binary episode`: episodes decodeed as having abstract states\n",
        "  \"\"\"\n",
        "  # return -(ml.predict_proba(episode)[0][1])\n",
        "  return ml.predict_proba(binary_episode)[0][0]\n",
        "\n",
        "def fitness_functional_probability(ml, binary_episode):\n",
        "  return ml.predict_proba(binary_episode)[0][0]\n",
        "\n",
        "\n",
        "def state_abstraction(model,state1,state2,d):\n",
        "  \"\"\"\n",
        "  This function compares to state, if they were in the same abstract class\n",
        "  function returs 'True' otherwise 'False'\n",
        "  \"\"\"\n",
        "  q_value1 = model.step_model.step([state1])\n",
        "  q_value2 = model.step_model.step([state2])\n",
        "  for i in range(len(q_value1[1][0])):\n",
        "    print(q_value1[1][0][i])\n",
        "    print(q_value2[1][0][i])\n",
        "    if ceil(q_value1[1][0][i]/d) == ceil(q_value2[1][0][i]/d):\n",
        "     continue\n",
        "    else:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def abstract_state(model,state1,d):\n",
        "  '''\n",
        "  works only for 2 actions \n",
        "  '''\n",
        "  if type(state1) == str:\n",
        "    if state1 == 'done':\n",
        "      return 'end'\n",
        "  q_value1 = model.step_model.step([state1])\n",
        "  return( ceil(q_value1[1][0][0]/d), ceil(q_value1[1][0][1]/d))\n",
        "\n",
        "def abstract_state_general(model,state1,d):\n",
        "  if type(state1) == str:\n",
        "    if state1 == 'done':\n",
        "      return 'end'\n",
        "  q_values = model.step_model.step([state1])\n",
        "  return tuple([ceil(q_value/d) for q_value in q_values[1][0]])\n",
        "\n",
        "\n",
        "#report function to check the performance metrics of the model\n",
        "def report(model2,x_train, y_train,x_test, y_test):\n",
        "  print(\"********************** reporting the result of the model **************************\")\n",
        "  print('The score for train data is {0}'.format(model2.score(x_train,y_train)))\n",
        "  print('The score for test data is {0}'.format(model2.score(x_test,y_test)))\n",
        "\n",
        "\n",
        "  predictions_train = model2.predict(x_train)\n",
        "  predictions_test = model2.predict(x_test)\n",
        "\n",
        "  print(\"\\n\\n--------------------------------------recall---------------------------------\")\n",
        "\n",
        "  print('the test recall for the class yes is {0}'.format(metrics.recall_score(y_test,predictions_test, pos_label=1)))\n",
        "  print('the test recall for the class no is {0}'.format(metrics.recall_score(y_test,predictions_test, pos_label=0)))\n",
        "\n",
        "  print('the training recall for the class yes is {0}'.format(metrics.recall_score(y_train,predictions_train, pos_label=1)))\n",
        "  print('the training recall for the class no is {0}'.format(metrics.recall_score(y_train,predictions_train, pos_label=0)))\n",
        "\n",
        "\n",
        "  print(\"\\n\\n--------------------------------------precision------------------------------\")\n",
        "\n",
        "\n",
        "  print('the test precision for the class yes is {0}'.format(metrics.precision_score(y_test,predictions_test, pos_label=1)))\n",
        "  print('the test precision for the class no is {0}'.format(metrics.precision_score(y_test,predictions_test, pos_label=0)))\n",
        "\n",
        "  print('the training precision for the class yes is {0}'.format(metrics.precision_score(y_train,predictions_train, pos_label=1)))\n",
        "  print('the training precision for the class no is {0}'.format(metrics.precision_score(y_train,predictions_train, pos_label=0)))\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "  print(classification_report(y_test, predictions_test, target_names=['NO ','yes']))\n",
        "\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, predictions_test).ravel()\n",
        "  specificity = tn / (tn+fp)\n",
        "  print(\"\\n\\nspecifity :\",specificity)\n",
        "  print(\"\\n\\n--------------------------------------confusion----------------------------\")\n",
        "  CM = metrics.confusion_matrix(y_test, predictions_test)\n",
        "  print(\"The confusion Matrix:\")\n",
        "  print(CM)\n",
        "  print('the accuracy score in {0}\\n\\n'.format(accuracy_score(y_test, predictions_test)))\n",
        "  print(\"********************** plotting the confusion matrix & ROC curve **************************\")\n",
        "  plot_confusion_matrix(model2, x_test, y_test)\n",
        "  metrics.plot_roc_curve(model2, x_test, y_test) \n",
        "  plt.show()\n",
        "\n",
        "#dump\n",
        "\n",
        "def dump_p(what, name):\n",
        "  with open(f'/content/drive/MyDrive/MC/{name}.pickle', 'wb') as file:\n",
        "      pickle.dump(what, file)\n",
        "\n",
        "\n",
        "# write function for load\n",
        "\n",
        "def load_p(name):\n",
        "  with open(f'/content/drive/MyDrive/MC/{name}.pickle', 'rb') as file2:\n",
        "    to_what = pickle.load(file2)\n",
        "  return to_what\n",
        "\n",
        "\n",
        "def random_test_1(model, env, Num):\n",
        "  obs=env.reset()\n",
        "  counter = 1\n",
        "  episode_reward = 0.0\n",
        "  for i in range(Num):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    # env.render()\n",
        "    episode_reward += reward\n",
        "    if done:\n",
        "      counter += 1\n",
        "      end = i\n",
        "      print(\"Reward:\", episode_reward, \"final state\", info['mem'][-2][0])\n",
        "      episode_reward = 0.0\n",
        "      obs = env.reset()\n",
        "  iter = deepcopy(counter)\n",
        "  u=1\n",
        "  while iter>1:\n",
        "    if info['mem'][-u][0]=='done':\n",
        "      lastpoint = -u\n",
        "      iter -= 1\n",
        "    u+=1\n",
        "  fin =Num - end\n",
        "  start = -Num -counter\n",
        "  randomtest = info['mem'][lastpoint:-fin]\n",
        "  ran_state = info['state'][(-counter+1):-1]\n",
        "  return randomtest , ran_state\n",
        "\n",
        "\n",
        "def fix_training(training_episodes,training_states):\n",
        "  buffer =[] \n",
        "  episodes_set = []\n",
        "  j=0\n",
        "  for i in range(len(training_episodes)):\n",
        "    if training_episodes[i][0] == 'done':\n",
        "      if i == 0:\n",
        "        continue\n",
        "      buffer.append(training_episodes[i])\n",
        "      episodes_set.append(buffer)\n",
        "      buffer=[]\n",
        "    else:\n",
        "      buffer.append(training_episodes[i])\n",
        "  if len(episodes_set)!=len(training_states):\n",
        "    del training_states[-1]\n",
        "  if len(episodes_set)!=len(training_states):\n",
        "    # assert False, 'problem in starting states'\n",
        "    print('problem in starting states')\n",
        "  return episodes_set , training_states\n",
        "\n",
        "def fix_testing(testing_episodes,testing_states,Env2):\n",
        "  buffer =[] \n",
        "  episodes_set = []\n",
        "  j=0\n",
        "  for i in range(len(testing_episodes)):\n",
        "    if testing_episodes[i][0] == 'done':\n",
        "      if i == 0:\n",
        "        continue\n",
        "      buffer.append(testing_episodes[i])\n",
        "      episodes_set.append(buffer)\n",
        "      buffer=[]\n",
        "    else:\n",
        "      buffer.append(testing_episodes[i])\n",
        "      # np.array(mtc_wrapped.set_state(qq[0]),dtype=\"float32\")\n",
        "  if not (episodes_set[0][0][0]==np.array(Env2.set_state(testing_states[0]),dtype=\"float32\")).all():\n",
        "    del testing_states[0]\n",
        "  if not (episodes_set[0][0][0]==np.array(Env2.set_state(testing_states[0]),dtype=\"float32\")).all():\n",
        "    assert False, 'problem in starting states'\n",
        "  if len(episodes_set)!=len(testing_states):\n",
        "    del testing_states[-1]\n",
        "  if len(episodes_set)!=len(testing_states):\n",
        "    assert False, 'problem in data prepration'\n",
        "  return episodes_set , testing_states\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqA8M1BJGJ2z"
      },
      "source": [
        "##ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VcT4ztE_GJ2z"
      },
      "outputs": [],
      "source": [
        "def Abstract_classes(ep,abstraction_d,model):\n",
        "  d=abstraction_d\n",
        "  abs_states1=[]\n",
        "  for episode in ep:\n",
        "    for state,action in episode:\n",
        "      abs_st = abstract_state_general(model,state,d)\n",
        "      if abs_st == 'end':\n",
        "        continue\n",
        "      abs_states1.append(abs_st)\n",
        "  unique1=list(set(abs_states1))\n",
        "  uni1 = np.array(unique1)\n",
        "  a=len(abs_states1)\n",
        "  b=len(set(abs_states1))\n",
        "  print(\"abstract states:\",b)\n",
        "  print(\"Concrete states\",a)\n",
        "  print(\"ratio\",b/a)\n",
        "  return unique1,uni1\n",
        "\n",
        "\n",
        "def ML_first_representation(Abs_d,epsilon_functional_fault_boarder,Reward_fault_boarder,uni1,model,ep,unique1):\n",
        "  \"\"\"\n",
        "  TO-DO : fix epsilon and threshold\n",
        "  \"\"\"\n",
        "  d = Abs_d\n",
        "  # epsilon = 0.05\n",
        "  epsilon = epsilon_functional_fault_boarder\n",
        "  data1_x_b=[]\n",
        "  data1_y_b= [] \n",
        "  data1_y_f_b = []\n",
        "  functional_fault = False\n",
        "  reward_fault_threshold =  Reward_fault_boarder\n",
        "\n",
        "  for episode in ep:\n",
        "    record = np.zeros(len(uni1))\n",
        "    for state, action in episode:\n",
        "      ab = abstract_state(model,state,d)\n",
        "      if ab == 'end':\n",
        "        print(action)\n",
        "        if functional_fault:\n",
        "          data1_y_f_b.append(1)\n",
        "        else:\n",
        "          data1_y_f_b.append(0)\n",
        "        if action >= reward_fault_threshold:\n",
        "          data1_y_b.append(0)\n",
        "        else:\n",
        "          data1_y_b.append(1)\n",
        "        functional_fault=False\n",
        "        continue\n",
        "      if state[0] < (-1.2+epsilon) :\n",
        "        # print(\"ff found\")\n",
        "        functional_fault = True\n",
        "        print(state[0])\n",
        "      ind = unique1.index(ab)\n",
        "      # if len(w[0])>1:\n",
        "        # print('error len is greater than 1')\n",
        "      record[ind] = 1\n",
        "      # if you want the frequency go with the next line \n",
        "      # record[ind] += 1\n",
        "    data1_x_b.append(record)\n",
        "\n",
        "  return data1_x_b, data1_y_b, data1_y_f_b\n",
        "\n",
        "def ML_first_representation_func_based(Abs_d,functional_func,reward_func,model,input_episodes,unique1):\n",
        "  \"\"\"\n",
        "  TO-DO : fix epsilon and threshold\n",
        "  \"\"\"\n",
        "  d = Abs_d\n",
        "  data1_x_b=[]\n",
        "  data1_y_b= [] \n",
        "  data1_y_f_b = []\n",
        "  for i, episode in enumerate(input_episodes):\n",
        "    record = np.zeros(len(unique1))\n",
        "    temp_flag = False\n",
        "    for state, action in episode:\n",
        "      ab = abstract_state_general(model,state,d)\n",
        "      if ab == 'end':\n",
        "        assert not temp_flag, f'Episode data problem, two terminations in one episode. Episode number{i}'\n",
        "        temp_flag = True\n",
        "        # print(action)\n",
        "        # print(functional_func(episode))\n",
        "        if functional_func(episode):\n",
        "          data1_y_f_b.append(1)\n",
        "        else:\n",
        "          data1_y_f_b.append(0)\n",
        "        if reward_func(episode):\n",
        "          data1_y_b.append(1)\n",
        "        else:\n",
        "          data1_y_b.append(0)\n",
        "        # print(\"end\\n\\n\\n\")\n",
        "        # print(len(data1_y_b),\"len(input_episodes)\",len(input_episodes))\n",
        "        continue\n",
        "        # print(state[0])\n",
        "      ind = unique1.index(ab)\n",
        "      record[ind] = 1\n",
        "      # print(state, action)\n",
        "      assert len(data1_y_b)<len(input_episodes), \"assert\"\n",
        "      # if you want the frequency go with the next line \n",
        "      # record[ind] += 1\n",
        "    data1_x_b.append(record)\n",
        "\n",
        "  return data1_x_b, data1_y_b, data1_y_f_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uMfthk5GJ2z"
      },
      "source": [
        "##Genetic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zEL89AH1GJ20"
      },
      "outputs": [],
      "source": [
        "def translator(episode,model, d, unique5):\n",
        "  \"\"\"\n",
        "  thid function takes the concrete episodes and returns the encoded episodes \n",
        "  based on the presence and absence of the individuals  \n",
        "  :param 'episode': input episode\n",
        "  :param 'model': RL model\n",
        "  :param 'd': abstraction level = 1\n",
        "  :param 'unique5': abstract classes \n",
        "  :return: encoded episodse based on the presence and absence\n",
        "\n",
        "  \"\"\"\n",
        "  d=d\n",
        "  record = np.zeros(len(unique5))\n",
        "  for state, action in episode:\n",
        "    ab = abstract_state_general(model,state,d)\n",
        "    if ab == 'end':\n",
        "      continue\n",
        "    if ab in unique5:\n",
        "      ind = unique5.index(ab)\n",
        "    record[ind] = 1\n",
        "  return [record]\n",
        "\n",
        "def transform(state):\n",
        "  position = state[0]\n",
        "  noise = np.random.uniform(low=0.95, high=1.05)\n",
        "  new_position= position * noise \n",
        "  new_state =deepcopy(state)\n",
        "  new_state[0] = new_position \n",
        "  return new_state\n",
        "\n",
        "\n",
        "def mutation_improved(population,model,env,objective_uncovered):\n",
        "  \"\"\"\n",
        "  This is the final mutation function \n",
        "  It takes the population as input and returns the mutated individual\n",
        "  :param 'population': Population that we want to mutate \n",
        "  :param 'model': RL model\n",
        "  :param 'env': RL environment\n",
        "  :param 'objective_uncovered: uncovered ubjectives for tournament selection\n",
        "  :return: mutated candidate (we re-rexecute the episode from the mutation part)\n",
        "  To-do:\n",
        "  move deepcopy to the cadidate class methods .set info \n",
        "  \"\"\"\n",
        "  parent = tournament_selection(population, 10, objective_uncovered)  # tournament selection\n",
        "  parent1 = deepcopy(parent.get_candidate_values())\n",
        "  if len(parent1) < 3:\n",
        "     assert False , \"parent in mutation is shorter than 3\"\n",
        "  Mutpoint = random.randint(3,(len(parent1)-3))\n",
        "  new_state = transform(parent1[Mutpoint][0])\n",
        "  action = model.predict(new_state)\n",
        "  if action[0]!= int(parent1[Mutpoint][1]):\n",
        "    print('Mutation lured the agent ... ')\n",
        "  new_parent = parent1[:Mutpoint]\n",
        "  new_parent.append([new_state,'Mut'])\n",
        "  new_cand =Candidate(new_parent)\n",
        "  new_cand.set_start_state(parent.get_start_state())\n",
        "\n",
        "  re_executed_epis = re_execute(model,env,new_cand)\n",
        "  \n",
        "  re_executed_cand = Candidate(re_executed_epis)\n",
        "  re_executed_cand.set_start_state(new_cand.get_start_state())\n",
        "  re_executed_cand.set_info(deepcopy(parent.get_info()))\n",
        "  re_executed_cand.set_info([\"mutation is done! \", \"mutpoint was:\",Mutpoint])\n",
        "\n",
        "  \n",
        "  return re_executed_cand\n",
        "\n",
        "def mutation_improved_p(parent,model,env,m_rate):\n",
        "  \"\"\"\n",
        "  This is the final mutation function with input of a parent considering internal m_rate\n",
        "  Here we give the parent to themutation funcion based on the given mutation \n",
        "  rate of m_rate, we may mutate the episodes. \n",
        "  :param 'parent' : individual that we want to mutate\n",
        "  :param 'model': RL model\n",
        "  :param 'env': RL environment\n",
        "  :param 'm_rate': mutation : recommended value is 1/len(parent)\n",
        "  :return : mutated individual\n",
        "  To-do:\n",
        "  move deepcopy to the cadidate .set info \n",
        "  \"\"\"\n",
        "  # parent = tournament_selection(population, 10, objective_uncovered)  # tournament selection\n",
        "  global MUTATION_NUMBER\n",
        "  chance = random.uniform(0, 1)\n",
        "  if chance> m_rate:\n",
        "    return parent\n",
        "  else:\n",
        "    # print(112)\n",
        "    parent1 = deepcopy(parent.get_candidate_values())\n",
        "    if len(parent1) < 3:\n",
        "      assert False , \"parent in mutation is shorter than 3\"\n",
        "    Mutpoint = random.randint(3,(len(parent1)-3))\n",
        "    new_state = transform(parent1[Mutpoint][0])\n",
        "    action = model.predict(new_state,deterministic=True)\n",
        "    if action[0]!= int(parent1[Mutpoint][1]):\n",
        "      print('Mutation lured the agent ... ')\n",
        "    new_parent = parent1[:Mutpoint]\n",
        "    new_parent.append([new_state,'Mut'])\n",
        "    new_cand =Candidate(new_parent)\n",
        "    new_cand.set_start_state(parent.get_start_state())\n",
        "    re_executed_epis = re_execute(model,env,new_cand)\n",
        "    n_reward = find_reward(re_executed_epis)\n",
        "    re_executed_epis[-1] = ('done',n_reward)\n",
        "    re_executed_cand = Candidate(re_executed_epis)\n",
        "    re_executed_cand.set_start_state(new_cand.get_start_state())\n",
        "    MUTATION_NUMBER+=1\n",
        "    return re_executed_cand\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Crossover_improved_v2(population,model,d,objective_uncovered):\n",
        "  \"\"\"\n",
        "  This is the crossover function that we are using \n",
        "  It takes the population as input and returns the mutated individual\n",
        "  :param 'population': Population. we select a parent based on the tournament\n",
        "   selection and then select the mutation point and then search for the matching point. \n",
        "  :param 'model': RL model\n",
        "  :param 'env': RL environment\n",
        "  :param 'objective_uncovered: uncovered ubjectives for tournament selection\n",
        "  :return: mutated candidate (we re-rexecute the episode from the mutation part)\n",
        "  To-do:\n",
        "  finding matching episode could be improved bu storing a mapping between concrete states and  \n",
        "  \"\"\"\n",
        "  found_match = False \n",
        "  while not (found_match):\n",
        "    parent = tournament_selection(population, 10, objective_uncovered)  # tournament selection\n",
        "    parent1 = deepcopy(parent.get_candidate_values())\n",
        "    parent1_start_point = deepcopy(parent.get_start_state())\n",
        "    if len(parent1)<4:\n",
        "      assert False, 'input of crossover is shorter than expected '\n",
        "    matches_list = []\n",
        "    crosspoint = random.randint(1,(len(parent1)-3))\n",
        "    abs_class = list(abstract_state_general(model,parent1[crosspoint][0],d))\n",
        "    for i in range(50):\n",
        "      indx = random.randint(0, len(population) - 1)\n",
        "      random_candidate = deepcopy(population[indx])\n",
        "      random_cand_data = random_candidate.get_candidate_values()\n",
        "      random_cand_start_point = random_candidate.get_start_state()\n",
        "      for st_index in range(1,len(random_cand_data)-3):\n",
        "        random_ab = list(abstract_state_general(model,random_cand_data[st_index][0],d))\n",
        "        if random_ab == abs_class:\n",
        "          matches_list.append(st_index)\n",
        "          found_match = True\n",
        "      if found_match:\n",
        "        break \n",
        "  # print('Crossover. attemp',i)\n",
        "  index_match_in_matchlist = random.randint(0, len(matches_list) - 1)\n",
        "  matchpoint = matches_list[index_match_in_matchlist]\n",
        "  match_candidate =  deepcopy(random_candidate)\n",
        "  match = deepcopy(random_cand_data)\n",
        "  match_start = deepcopy(random_cand_start_point)\n",
        "  offspring1 = deepcopy(parent1[:crosspoint])\n",
        "  offspring1 += deepcopy(match[matchpoint:])\n",
        "  new_reward1  = find_reward(offspring1)\n",
        "  offspring1[-1] = ('done',new_reward1)\n",
        "  candid1 = Candidate(offspring1)\n",
        "  candid1.set_start_state(parent1_start_point)\n",
        "  offspring2 = deepcopy(match[:matchpoint])\n",
        "  offspring2 += deepcopy(parent1[crosspoint:])\n",
        "  new_reward2  = find_reward(offspring2)\n",
        "  offspring2[-1] = ('done',new_reward2)\n",
        "  candid2 = Candidate(offspring2)\n",
        "  candid2.set_start_state(match_start)\n",
        "  if len(offspring1)<4:\n",
        "    print(offspring1)\n",
        "    assert False, 'created offspring 1 in crossover is shorter than expected '\n",
        "\n",
        "  if len(offspring2)<4:\n",
        "    print(offspring2)\n",
        "    assert False, 'created offspring 2 in crossover is shorter than expected '\n",
        "\n",
        "  return candid1, candid2\n",
        "\n",
        "def find_reward(episode):\n",
        "  if len(episode)>200:\n",
        "    return -200\n",
        "  if len(episode)<=200:\n",
        "    if is_functional_fault_last_state(episode[-2],episode[-1]):\n",
        "      return -200\n",
        "    else:\n",
        "      return -(len(episode)-1)\n",
        "\n",
        "def Crossover_improved_v2_random(population,model,d,objective_uncovered):\n",
        "  found_match = False \n",
        "  while not found_match:\n",
        "    i = random.randint(0, len(population))\n",
        "    parent1 = deepcopy(population[i].get_candidate_values())\n",
        "    parent1_start_point = deepcopy(population[i].get_start_state())\n",
        "    matches_list = []\n",
        "    crosspoint = random.randint(1,(len(parent1)-3))\n",
        "    abs_class = list(abstract_state(model,parent1[crosspoint][0],d))\n",
        "    attemp = 0\n",
        "    for i in range(700):\n",
        "      attemp +=1\n",
        "      indx = random.randint(0, len(population) - 1)\n",
        "      random_candidate = deepcopy(population[indx])\n",
        "      random_cand_data = random_candidate.get_candidate_values()\n",
        "      random_cand_start_point = random_candidate.get_start_state()\n",
        "      for st_index in range(1,len(random_cand_data)-3):\n",
        "        random_ab = list(abstract_state(model,random_cand_data[st_index][0],d))\n",
        "        if random_ab == abs_class:\n",
        "          matches_list.append(st_index)\n",
        "          found_match = True\n",
        "      if found_match:\n",
        "        break \n",
        "  print(\"match found in --- attemps\",attemp)\n",
        "  index_match_in_matchlist = random.randint(0, len(matches_list) - 1)\n",
        "  matchpoint = matches_list[index_match_in_matchlist]\n",
        "  match_candidate = random_candidate\n",
        "  match = random_cand_data\n",
        "  match_start = deepcopy(random_cand_start_point)\n",
        "  offspring1 = deepcopy(parent1[:crosspoint])\n",
        "  offspring1 += deepcopy(match[matchpoint:])\n",
        "  offspring1[-1] = ['done',(len(offspring1)-1)]\n",
        "  candid1 = Candidate(offspring1)\n",
        "  candid1.set_start_state(parent1_start_point)\n",
        "\n",
        "  offspring2 = deepcopy(match[:matchpoint])\n",
        "  offspring2 += deepcopy(parent1[crosspoint:])\n",
        "  offspring2[-1] = ['done',(len(offspring2)-1)]\n",
        "  candid2 = Candidate(offspring2)\n",
        "  candid2.set_start_state(match_start)\n",
        "  return candid1, candid2\n",
        "\n",
        "#updated for Mountain car\n",
        "def re_execute(model,env,candidate):\n",
        "  obs =env.reset()\n",
        "  obs =env.set_state(deepcopy(candidate.get_start_state()))\n",
        "  episode = candidate.get_candidate_values()\n",
        "  steps_to_mut_point = len(episode)\n",
        "  episode_reward = 0.0\n",
        "  done= False \n",
        "  counter = 0 \n",
        "  for i in range(steps_to_mut_point):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    action_selected = episode[i][1]\n",
        "    if action_selected == 'Mut':\n",
        "      # print(episode[i])\n",
        "      # print(episode[i][0])\n",
        "      action_selected, _ = model.predict(episode[i][0], deterministic=True)\n",
        "      # print(\"ddd\",i,\"eee\",steps_to_mut_point)\n",
        "      # print(action_selected)\n",
        "      # break\n",
        "    obs, reward, done, info = env.step(int(action_selected)) # its very important to select the action here it means that we may \n",
        "    counter+=1\n",
        "    #follow the previous path until the mutation point or we follow the route that the trained agent wants to follow forcing vs following \n",
        "    episode_reward += reward\n",
        "    # print(\"counter\",counter)\n",
        "    if done:\n",
        "      break \n",
        "  for j in range(200):\n",
        "    if done:\n",
        "      break\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = env.step(action) \n",
        "    counter+=1\n",
        "    episode_reward += reward\n",
        "  assert done\n",
        "  if episode_reward>201:\n",
        "    assert False \n",
        "  return env.info['mem'][-((counter)+1):]\n",
        "\n",
        "\n",
        "def re_execution_improved(model,env,candidate):\n",
        "  differences=[]\n",
        "  episode_limit = 200 \n",
        "  env.reset()\n",
        "  obs =env.set_state(candidate.get_start_state()) \n",
        "  episode = candidate.get_candidate_values()\n",
        "  # steps_to_mut_point = len(episode)\n",
        "  episode_reward = 0.0\n",
        "  for i in range(episode_limit):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    action_selected = episode[i][1]\n",
        "    if  episode[i][0]=='done':\n",
        "      continue\n",
        "    if i >=len(episode):\n",
        "      action, _ = model.predict(obs, deterministic=True)\n",
        "      obs, reward, done, info = env.step(int(action)) \n",
        "      continue\n",
        "    if action != int(action_selected):\n",
        "      prob=model.action_probability(episode[i][0])\n",
        "      differences.append([i , prob])\n",
        "    obs, reward, done, info = env.step(int(action_selected))\n",
        "    # env.render()\n",
        "    # env.reset = state1\n",
        "    episode_reward += reward\n",
        "    if done:\n",
        "      # assert not done\n",
        "      break \n",
        "  assert done , \"not finished in 2oo steps \"\n",
        "  return differences\n",
        "\n",
        "\n",
        "def re_execution_improved_v2(model,env,candidate):\n",
        "  differences=[]\n",
        "  episode_limit = 200 \n",
        "  env.reset()\n",
        "  obs =env.set_state(candidate.get_start_state()) \n",
        "  episode = candidate.get_candidate_values()\n",
        "  episode_reward = 0.0\n",
        "  for i in range(episode_limit):\n",
        "    if i >=(len(episode)-1):\n",
        "      action, _ = model.predict(obs, deterministic=True)\n",
        "      obs, reward, done, info = env.step(int(action)) \n",
        "      if done:\n",
        "      # assert not done\n",
        "        print(\"Reward:\", episode_reward)\n",
        "        # break\n",
        "        return differences \n",
        "      continue\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    if  episode[i][0]=='done':\n",
        "      print(\"first scenario, episode finished correctly\")\n",
        "      # continue\n",
        "    print(len(episode),i)\n",
        "    action_selected = episode[i][1]\n",
        "    if action != int(action_selected):\n",
        "      prob=model.action_probability(episode[i][0])\n",
        "      differences.append([i , prob])\n",
        "    obs, reward, done, info = env.step(int(action_selected)) \n",
        "    # env.render()\n",
        "    # env.reset = state1\n",
        "    episode_reward += reward\n",
        "    if done:\n",
        "      # assert not done\n",
        "      break \n",
        "  assert done , \"not finished in 2oo steps \"\n",
        "  return differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rCMMfPALGJ20"
      },
      "outputs": [],
      "source": [
        "#changed\n",
        "import numpy as np\n",
        "class Candidate:\n",
        "    def __init__(self, candidates_vals):\n",
        "        if isinstance(candidates_vals, (np.ndarray, np.generic)):\n",
        "            self.candidate_values = candidates_vals.tolist()\n",
        "        else:\n",
        "            self.candidate_values = candidates_vals\n",
        "        self.objective_values = []\n",
        "        self.objectives_covered = []\n",
        "        self.crowding_distance = 0\n",
        "        self.uncertainity = []\n",
        "        self.start_state = 0\n",
        "        self.information = []\n",
        "        self.mutation = False\n",
        "\n",
        "    def get_candidate_values(self):\n",
        "        return self.candidate_values\n",
        "\n",
        "    def get_uncertainity_value(self, indx):\n",
        "        return self.uncertainity[indx]\n",
        "    def get_uncertainity_values(self):\n",
        "        return self.uncertainity\n",
        "    def set_uncertainity_values(self,uncertain):\n",
        "        self.uncertainity = uncertain\n",
        "    def set_candidate_values(self, cand):\n",
        "        self.candidate_values = cand\n",
        "    def set_candidate_values_at_index(self, indx,val):\n",
        "        self.candidate_values[indx] = val\n",
        "\n",
        "    def get_objective_values(self):\n",
        "        return self.objective_values\n",
        "\n",
        "    def get_objective_value(self, indx):\n",
        "        return self.objective_values[indx]\n",
        "\n",
        "    def set_objective_values(self, obj_vals):\n",
        "        self.objective_values = obj_vals\n",
        "\n",
        "    def add_objectives_covered(self, obj_covered):\n",
        "        if obj_covered not in self.objectives_covered:\n",
        "            self.objectives_covered.append(obj_covered)\n",
        "\n",
        "    def get_covered_objectives(self):\n",
        "        return self.objectives_covered\n",
        "\n",
        "    def set_crowding_distance(self, cd):\n",
        "        self.crowding_distance = cd\n",
        "\n",
        "    def get_crowding_distance(self):\n",
        "        return self.crowding_distance\n",
        "\n",
        "    def exists_in_satisfied(self, indx):\n",
        "        for ind in self.objectives_covered:\n",
        "            if ind == indx:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def is_objective_covered(self, obj_to_check):\n",
        "        for obj in self.objectives_covered:\n",
        "            if obj == obj_to_check:\n",
        "                return True\n",
        "        return False\n",
        "    def set_start_state(self,start_point):\n",
        "      self.start_state = deepcopy(start_point)\n",
        "\n",
        "    def get_start_state(self):\n",
        "      return self.start_state\n",
        "\n",
        "    def set_info(self, new_information):\n",
        "      self.information.append(new_information)\n",
        "      \n",
        "    def get_info(self):\n",
        "      return self.information\n",
        "\n",
        "    def mutated(self):\n",
        "      self.mutation = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dg-62_5gGJ20"
      },
      "outputs": [],
      "source": [
        "def mutation_number_update(file_address,Mut_Num_to_add,iteration):\n",
        "  if iteration == 0:\n",
        "    with open(file_address, 'wb') as file:\n",
        "      pickle.dump(Mut_Num_to_add, file)\n",
        "    return\n",
        "  with open(file_address, 'rb') as file2:\n",
        "    Mut_num = pickle.load(file2)\n",
        "  print(Mut_num)\n",
        "  if type(Mut_num) == list:\n",
        "    print('list')\n",
        "    buffer = Mut_num\n",
        "    buffer.append(Mut_Num_to_add)\n",
        "    print(buffer)\n",
        "  else:\n",
        "    print('int')\n",
        "    buffer =[] \n",
        "    buffer.append(Mut_num)\n",
        "    buffer.append(Mut_Num_to_add)\n",
        "    print(buffer)\n",
        "  with open(file_address, 'wb') as file:\n",
        "    pickle.dump(buffer, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kYTYP6MGJ20"
      },
      "source": [
        "##MOSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rcBCi-cfGJ21"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.StandardScaler()\n",
        "\n",
        "\n",
        "\n",
        "# domination relation method, same as MOSA \n",
        "def dominates(value_from_pop, value_from_archive, objective_uncovered):\n",
        "    dominates_f1 = False\n",
        "    dominates_f2 = False\n",
        "    for each_objective in objective_uncovered:\n",
        "        f1 = value_from_pop[each_objective]\n",
        "        f2 = value_from_archive[each_objective]\n",
        "        if f1 < f2:\n",
        "            dominates_f1 = True\n",
        "        if f2 < f1:\n",
        "            dominates_f2 = True\n",
        "        if dominates_f1 and dominates_f2:\n",
        "            break\n",
        "    if dominates_f1 == dominates_f2:\n",
        "        return False\n",
        "    elif dominates_f1:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# calculating the fitness value function\n",
        "\n",
        "def evaulate_population(func, pop , parameters):\n",
        "    for candidate in pop:\n",
        "      if isinstance(candidate, Candidate):\n",
        "        # print(candidate.get_candidate_values())\n",
        "        result = func(candidate.get_candidate_values())\n",
        "        candidate.set_objective_values(result)\n",
        "        print(candidate.get_objective_values())\n",
        "\n",
        "def evaulate_population_with_archive(func, pop, already_executed):\n",
        "    to_ret = []\n",
        "    for candidate in pop:\n",
        "        if isinstance(candidate, Candidate):\n",
        "            if candidate.get_candidate_values() in already_executed:\n",
        "                continue\n",
        "\n",
        "            result = func(candidate.get_candidate_values())\n",
        "            candidate.set_objective_values(result)\n",
        "            already_executed.append(candidate.get_candidate_values())\n",
        "            to_ret.append(candidate)\n",
        "    return to_ret\n",
        "\n",
        "def exists_in_archive(archive, index):\n",
        "    for candidate in archive:\n",
        "        if candidate.exists_in_satisfied(index):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# searching archive\n",
        "def get_from_archive(obj_index, archive):\n",
        "    for candIndx in range(len(archive)):\n",
        "        candidate = archive[candIndx]\n",
        "        if candidate.exists_in_satisfied(obj_index):\n",
        "            return candidate, candIndx\n",
        "    return None\n",
        "\n",
        "\n",
        "# updating archive with adding the number of objective it satisfies, Same as Mosa paper\n",
        "def update_archive(pop, objective_uncovered, archive, no_of_Objectives, threshold_criteria):\n",
        "    for objective_index in range(no_of_Objectives):\n",
        "        for pop_index in range(len(pop)):\n",
        "            objective_values = pop[pop_index].get_objective_values()\n",
        "            # if not objective_values[objective_index] or not threshold_criteria[objective_index]:\n",
        "            if objective_values[objective_index] <= threshold_criteria[objective_index]:\n",
        "                if exists_in_archive(archive, objective_index):\n",
        "                    archive_value, cand_indx = get_from_archive(objective_index, archive)\n",
        "                    obj_archive_values = archive_value.get_objective_values()\n",
        "                    if obj_archive_values[objective_index] > objective_values[objective_index]:\n",
        "                        value_to_add = pop[pop_index]\n",
        "                        value_to_add.add_objectives_covered(objective_index)\n",
        "                        # archive.append(value_to_add)\n",
        "                        archive[cand_indx] = value_to_add\n",
        "                        if objective_index in objective_uncovered:\n",
        "                            objective_uncovered.remove(objective_index)\n",
        "                        # archive.remove(archive_value)\n",
        "                else:\n",
        "                    value_to_add = pop[pop_index]\n",
        "                    value_to_add.add_objectives_covered(objective_index)\n",
        "                    archive.append(value_to_add)\n",
        "                    if objective_index in objective_uncovered:\n",
        "                        objective_uncovered.remove(objective_index)\n",
        "\n",
        "\n",
        "# method to get the most dominating one\n",
        "def select_best(tournament_candidates, objective_uncovered):\n",
        "    best = tournament_candidates[0]  # in case none is dominating other\n",
        "    for i in range(len(tournament_candidates)):\n",
        "        candidate1 = tournament_candidates[i]\n",
        "        for j in range(len(tournament_candidates)):\n",
        "            candidate2 = tournament_candidates[j]\n",
        "            if (dominates(candidate1.get_objective_values(), candidate2.get_objective_values(), objective_uncovered)):\n",
        "                best = candidate1\n",
        "    return best\n",
        "\n",
        "\n",
        "def tournament_selection_improved(pop, size, objective_uncovered):\n",
        "    tournament_candidates = []\n",
        "    for i in range(size):\n",
        "        indx = random.randint(0, len(pop) - 1)\n",
        "        random_candidate = pop[indx]\n",
        "        tournament_candidates.append(random_candidate)\n",
        "\n",
        "    best = select_best(tournament_candidates, objective_uncovered)\n",
        "    return best;\n",
        "\n",
        "\n",
        "def tournament_selection(pop, size, objective_uncovered):\n",
        "    tournament_candidates = []\n",
        "    for i in range(size):\n",
        "        indx = random.randint(0, len(pop) - 1)\n",
        "        random_candidate = pop[indx]\n",
        "        tournament_candidates.append(random_candidate)\n",
        "\n",
        "    best = select_best(tournament_candidates, objective_uncovered)\n",
        "    return best;\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_offspring_improved(population,model,env,d,objective_uncovered):\n",
        "    population_to_return = []\n",
        "    probability_C = 0.75\n",
        "    probability_M = 0.3\n",
        "    size = len(population)\n",
        "    while (len(population_to_return) < size):\n",
        "      probability_crossover = random.uniform(0, 1)\n",
        "      if probability_crossover <= probability_C:  # 75% probability\n",
        "        off1, off2 = Crossover_improved_v2(population,model,1,objective_uncovered)\n",
        "        population_to_return.append(off1)\n",
        "        population_to_return.append(off2)\n",
        "      probability_mutation = random.uniform(0, 1)\n",
        "      if probability_mutation <= probability_M:  # 30% probability this in for test purposes \n",
        "        off3 = mutation_improved(population, model,env,objective_uncovered)\n",
        "        population_to_return.append(off3)\n",
        "    return population_to_return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_offspring_improved_v2(population,model,env,d,objective_uncovered):\n",
        "    \n",
        "    population_to_return = []\n",
        "    probability_C = 0.75\n",
        "    probability_M = 0.01\n",
        "    size = len(population)\n",
        "    while (len(population_to_return) < size):\n",
        "      probability_crossover = random.uniform(0, 1)\n",
        "      if probability_crossover <= probability_C:  # 75% probability\n",
        "        parent1, parent2 = Crossover_improved_v2(population,model,d,objective_uncovered)\n",
        "        parent1 = mutation_improved_p(parent1, model,env, (1 / len(parent1.get_candidate_values())))\n",
        "        parent2 = mutation_improved_p(parent2, model,env, (1 / len(parent2.get_candidate_values())))\n",
        "        population_to_return.append(parent1)\n",
        "        population_to_return.append(parent2)\n",
        "\n",
        "      if probability_crossover > probability_C:\n",
        "        parent = tournament_selection(population, 10, objective_uncovered) #we may add a very small number of duplicated individulas but its not important as we are removing them in the final executions\n",
        "        population_to_return.append(mutation_improved_p(parent, model,env,(1 / len(parent.get_candidate_values())))) \n",
        "      \n",
        "\n",
        "    return population_to_return\n",
        "\n",
        "def save_all_data(pop,no_of_Objectives,threshold_criteria, stored_data):\n",
        "  '''\n",
        "  This function will save all individulas with objective lower than treshhold \n",
        "\n",
        "  '''\n",
        "  threshold_criteria_to_add_to_archive = [70, 0.06, 0.05, 0.05] \n",
        "  # be careful here ypu can set the satisfiing objectives that based on them you want to store the data  \n",
        "  for individual in pop:\n",
        "    individual_objective = individual.get_objective_values()\n",
        "    for i in range(no_of_Objectives):\n",
        "      if individual_objective[i]<threshold_criteria_to_add_to_archive[i]:\n",
        "        # if individual not in stored_data:\n",
        "        #   ind_ = deepcopy(individual)\n",
        "        #   stored_data.append(ind_)\n",
        "        # individual_objective_values = individual.get_objective_values()\n",
        "        found = False\n",
        "        for j in range(len(stored_data)):\n",
        "          if individual_objective == stored_data[j].get_objective_values():\n",
        "            found = True\n",
        "            break\n",
        "        if not found:\n",
        "          ind_ = deepcopy(individual)\n",
        "          stored_data.append(ind_)\n",
        "  # return stored_data\n",
        "\n",
        "def save_all_data2(pop, stored_data):\n",
        "  '''\n",
        "  This function will save all individulas in generations \n",
        "  you need to remove redundant data (based on fitness and ...)\n",
        "\n",
        "  '''\n",
        "  stored_data.append(list(pop))\n",
        "\n",
        "\n",
        "def Build_Archive(pop,no_of_Objectives,threshold_criteria, stored_data, initial_population):\n",
        "  '''\n",
        "  If you are using the Archive of all generated episodes, this function\n",
        "  removes the duplicated results and builds the Archive.\n",
        "  :param 'pop': current generation\n",
        "  :param 'no_of_Objectives': number of objectives\n",
        "  :param 'threshold_criteria': threshold criteria (we are intrested in episodes that have fitness below these threshold values)\n",
        "  :param 'stored_data': Archive of final episodes (return)\n",
        "  :param 'initial_population': initial population. we are not considering these episodes in our archive for the second senario you need to add the number of faults, (implementation in RQ3)\n",
        "  '''\n",
        "  threshold_criteria_to_add_to_archive = threshold_criteria\n",
        "# be careful as we can have different values for criterias here to add episodes to archive and for GA stopping criteria \n",
        "  for individual in pop:\n",
        "    individual_objective = individual.get_objective_values()\n",
        "    for i in range(no_of_Objectives):\n",
        "      if individual_objective[i]<threshold_criteria_to_add_to_archive[i]:\n",
        "        found = False\n",
        "        for j in range(len(stored_data)):\n",
        "          if individual_objective == stored_data[j].get_objective_values():\n",
        "            found = True\n",
        "            break\n",
        "        for k in range(len(initial_population)):\n",
        "          if individual_objective == initial_population[k].get_objective_values():\n",
        "            found = True\n",
        "            break\n",
        "        if not found:\n",
        "          ind_ = deepcopy(individual)\n",
        "          stored_data.append(ind_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaTahUHqGJ22"
      },
      "source": [
        "###Sorting and RUN search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "azVEVOUnGJ22"
      },
      "outputs": [],
      "source": [
        "\n",
        "# finding best candidates and assigning to each front\n",
        "def fast_dominating_sort(R_T, objective_uncovered):\n",
        "    to_return = []\n",
        "    front = []\n",
        "    count = 0\n",
        "    while len(R_T) > 1:\n",
        "        count = 0\n",
        "        for outer_loop in range(len(R_T)):\n",
        "            best = R_T[outer_loop]\n",
        "            add = True\n",
        "            for inner_loop in range(len(R_T)):\n",
        "                against = R_T[inner_loop]\n",
        "                if best == against:\n",
        "                    continue\n",
        "                if (dominates(best.get_objective_values(), against.get_objective_values(), objective_uncovered)):\n",
        "                    continue\n",
        "                else:\n",
        "                    add = False\n",
        "                    break\n",
        "\n",
        "            if add == True:\n",
        "                if best not in front:\n",
        "                    front.append(best)\n",
        "\n",
        "                count = count + 1\n",
        "\n",
        "        if len(front) > 0:\n",
        "            to_return.append(front)\n",
        "            for i in range(len(front)):\n",
        "                R_T.remove(front[i])\n",
        "                front = []\n",
        "\n",
        "        if (len(to_return) == 0) or (count == 0):  # to check if no one dominates no one\n",
        "            to_return.append(R_T)\n",
        "            break\n",
        "\n",
        "    return to_return\n",
        "\n",
        "\n",
        "# sorting based on crowding distance\n",
        "def sort_based_on_crowding_distance(e):\n",
        "    values = e.get_crowding_distance()\n",
        "    return values\n",
        "\n",
        "\n",
        "def sort_based_on(e):\n",
        "    values = e.get_objective_values()\n",
        "    return values[0]\n",
        "\n",
        "\n",
        "# sorting based on first objective value\n",
        "def sort_worse(pop):\n",
        "    pop.sort(key=sort_based_on, reverse=True)\n",
        "    return pop\n",
        "# preference sort, same as algorithm\n",
        "def preference_sort(R_T, size, objective_uncovered):\n",
        "    to_return = []\n",
        "    for objective_index in objective_uncovered:\n",
        "        min = 100\n",
        "        best = R_T[0]\n",
        "        for index in range(len(R_T)):\n",
        "            objective_values = R_T[index].get_objective_values()\n",
        "            if objective_values[objective_index] < min:\n",
        "                min = objective_values[objective_index]\n",
        "                best = R_T[index]\n",
        "        to_return.append(best)\n",
        "        R_T.remove(best)\n",
        "    if len(R_T)>0:\n",
        "        E = fast_dominating_sort(R_T, objective_uncovered)\n",
        "        for i in range(len(E)):\n",
        "            to_return.append(E[i])\n",
        "    return to_return\n",
        "\n",
        "\n",
        "# converting to numpy array (Required by library)\n",
        "def get_array_for_crowding_distance(sorted_front):\n",
        "    list = []\n",
        "    for value in sorted_front:\n",
        "        objective_values = value.get_objective_values()\n",
        "\n",
        "        np_array = numpy.array(objective_values)\n",
        "        list.append(np_array)\n",
        "\n",
        "    np_list = np.array(list)\n",
        "    cd = calc_crowding_distance(np_list)\n",
        "    return cd\n",
        "# method to assign each candidate its crownding distance\n",
        "\n",
        "def assign_crowding_distance_to_each_value(sorted_front, crowding_distance):\n",
        "    for candidate_index in range(len(sorted_front)):\n",
        "        objective_values = sorted_front[candidate_index]\n",
        "        objective_values.set_crowding_distance(crowding_distance[candidate_index])\n",
        "\n",
        "def run_search(func, initial_population, no_of_Objectives, criteria,archive,logger,start,time_budget,size,d,env, parameters , second_archive,gens):\n",
        "    global MUTATION_NUMBER\n",
        "    MUTATION_NUMBER=0\n",
        "    threshold_criteria = criteria \n",
        "    objective_uncovered = []\n",
        "    print(\"initial population \",type(initial_population),len(initial_population))\n",
        "\n",
        "    for obj in range(no_of_Objectives):\n",
        "        objective_uncovered.append(obj)  # initializing number of uncovered objective\n",
        "\n",
        "    random_population = initial_population \n",
        "\n",
        "    P_T = copy.copy(random_population)\n",
        "    evaulate_population(func, random_population ,parameters)  # evaluating whole generation and storing results propabibly its with candidates\n",
        "\n",
        "    # print(random_population[0].get_objective_values())\n",
        "    update_archive(random_population, objective_uncovered, archive, no_of_Objectives,threshold_criteria)  # updating archive \n",
        "    # save initial population\n",
        "    save_all_data2(random_population,gens)\n",
        "    iteration = 0\n",
        "    #limit of number of generations \n",
        "    while iteration <10:\n",
        "        iteration = iteration + 1  # iteration count\n",
        "        #To-DO: limit by the time budget instead of the generation number\n",
        "        for arc in archive:\n",
        "            logger.info(\"***ARCHIVE***\")\n",
        "            logger.info(\"\\nValues: \" + str(\n",
        "                arc.get_candidate_values()) + \"\\nwith objective values: \" + str(\n",
        "                arc.get_objective_values()) + \"\\nSatisfying Objective: \" + str(\n",
        "                arc.get_covered_objectives()))\n",
        "        print(\"Iteration count: \" + str(iteration))\n",
        "        logger.info(\"Iteration is : \" + str(iteration))\n",
        "        logger.info(\"Number of mutations : \" + str(MUTATION_NUMBER))\n",
        "\n",
        "        R_T = []\n",
        "        \n",
        "        Q_T = generate_offspring_improved_v2(P_T,model,env,d,objective_uncovered) #generate offsprings using crossover and mutation \n",
        "\n",
        "        evaulate_population(func, Q_T, parameters)  # evaluating offspring\n",
        "        update_archive(Q_T, objective_uncovered, archive, no_of_Objectives, threshold_criteria)  # updating archive\n",
        "        save_all_data(Q_T,no_of_Objectives,threshold_criteria,second_archive)\n",
        "        # save generations\n",
        "        save_all_data2(Q_T,gens)\n",
        "        R_T = copy.deepcopy(P_T)  # R_T = P_T union Q_T\n",
        "        R_T.extend(Q_T)\n",
        "\n",
        "        F = preference_sort(R_T, size, objective_uncovered)  # Preference sorting and getting fronts\n",
        "\n",
        "        if len(objective_uncovered) == 0:  # checking if all objectives are covered\n",
        "            print(\"all_objectives_covered\")\n",
        "            logger.info(\"***Final-ARCHIVE***\")\n",
        "            print((\"***Final-ARCHIVE***\"))\n",
        "            for arc in archive:\n",
        "                print(\"\\nValues: \" + str(\n",
        "                    arc.get_candidate_values()) + \"\\nwith objective values: \" + str(\n",
        "                    arc.get_objective_values()) + \"\\nSatisfying Objective: \" + str(\n",
        "                    arc.get_covered_objectives()))\n",
        "\n",
        "                logger.info(\"\\nValues: \" + str(\n",
        "                    arc.get_candidate_values()) + \"\\nwith objective values: \" + str(\n",
        "                    arc.get_objective_values()) + \"\\nSatisfying Objective: \" + str(\n",
        "                    arc.get_covered_objectives()))\n",
        "            logger.info(\"Iteration is : \"+str(iteration))\n",
        "            logger.info(\"Number of mutations : \"+str(MUTATION_NUMBER))\n",
        "            break\n",
        "\n",
        "        P_T_1 = []  # creating next generatint PT+1\n",
        "        index = 0\n",
        "\n",
        "        while len(P_T_1) <= size:  # if length of current generation is less that size of front at top then add it\n",
        "\n",
        "            if not isinstance(F[index], Candidate):\n",
        "                if len(P_T_1) + len(F[index]) > size:\n",
        "                    break\n",
        "            else:\n",
        "                if len(P_T_1) + 1 > size:\n",
        "                    break\n",
        "\n",
        "            front = F[index]\n",
        "            if isinstance(F[index], Candidate):  # if front contains only one item\n",
        "                P_T_1.append(F[index])\n",
        "                F.remove(F[index])\n",
        "            else:\n",
        "                for ind in range(len(F[index])):  # if front have multiple items\n",
        "                    val = F[index][ind]\n",
        "                    P_T_1.append(val)\n",
        "\n",
        "                F.remove(F[index])\n",
        "        while (len(P_T_1)) < size:  # crowding distance\n",
        "            copyFront = copy.deepcopy(F[index])\n",
        "            sorted_front = sort_worse(copyFront)  # sort before crowding distance\n",
        "\n",
        "            crowding_distance = get_array_for_crowding_distance(sorted_front)  # coverting to libaray compaitble array\n",
        "            assign_crowding_distance_to_each_value(sorted_front,\n",
        "                                                   crowding_distance)  # assinging each solution its crowding distance\n",
        "            sorted_front.sort(key=sort_based_on_crowding_distance, reverse=True)  # sorting based on crowding distance\n",
        "\n",
        "            if (len(sorted_front) + len(\n",
        "                    P_T_1)) > size:  # maintaining length and adding solutions with most crowding distances\n",
        "                for sorted_front_indx in range(len(sorted_front)):\n",
        "                    candidate = sorted_front[sorted_front_indx]\n",
        "                    P_T_1.append(candidate)\n",
        "                    if len(P_T_1) >= size:\n",
        "                        break\n",
        "\n",
        "            index = index + 1\n",
        "\n",
        "        P_T_1 = P_T_1[0:size]\n",
        "        P_T = P_T_1  # assigning PT+1 to PT\n",
        "\n",
        "\n",
        "def minimize(func, population, lb, ub, no_of_Objectives, criteria,time_budget,logger,archive,size,d,env,parameters, second_archive,gens):\n",
        "    assert hasattr(func, '__call__')\n",
        "\n",
        "    start = time.time()\n",
        "    run_search(func, population, no_of_Objectives, criteria,archive,logger,start,time_budget,size,d,env ,parameters, second_archive,gens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N0NKW6cOGJ22"
      },
      "outputs": [],
      "source": [
        "class CartPole_caseStudy():\n",
        "    def __init__(self):\n",
        "        logger = logging.getLogger()\n",
        "\n",
        "        now = datetime.now()\n",
        "        log_file = 'output/STARLA' + str(i) + '_V2' + str(now) + '.log'\n",
        "        logging.basicConfig(filename=log_file,\n",
        "                            format='%(asctime)s %(message)s')\n",
        "        self.parameters = [model,d,unique5]\n",
        "        logger.setLevel(logging.WARNING)\n",
        "    def _evaluate(self,x):\n",
        "        fv = x\n",
        "        model,d,unique5 = self.parameters\n",
        "        obj1 = fitness_reward(fv)\n",
        "        obj2 = fitness_confidence(fv,model,'m')\n",
        "        binary_fv = translator(fv,model,d,unique5)\n",
        "        obj3 = fitness_functional_probability(RF_FF_1rep,binary_fv)\n",
        "        obj4 = fitness_functional_probability(RF_RF_1rep,binary_fv)\n",
        "        to_ret = [obj1,obj2,obj3,obj4]\n",
        "        logger = logging.getLogger()\n",
        "        logger.info(str(fv)+\",\"+str(to_ret))\n",
        "        return to_ret\n",
        "\n",
        "\n",
        "class MountainCar_caseStudy():\n",
        "    def __init__(self):\n",
        "        logger = logging.getLogger()\n",
        "        now = datetime.now()\n",
        "        log_file = 'log/STARLA' + str(i) + '_V2' + str(now) + '.log'\n",
        "        logging.basicConfig(filename=log_file,\n",
        "                            format='%(asctime)s %(message)s')\n",
        "        self.parameters = [model,d,unique5]\n",
        "        logger.setLevel(logging.WARNING)\n",
        "    def _evaluate(self,x):\n",
        "        fv = x\n",
        "        model,d,unique5 = self.parameters\n",
        "        obj1 = fitness_reward_general(fv)\n",
        "        if obj1==None:\n",
        "          debug_data1=[fv,x]\n",
        "          with open(f'/content/drive/MyDrive/debug/data.pickle', 'wb') as file:\n",
        "              pickle.dump(debug_data1, file)\n",
        "          assert False\n",
        "        obj2 = fitness_confidence_general(fv,model,'m')\n",
        "        binary_fv = translator(fv,model,d,unique5)\n",
        "        obj3 = fitness_functional_probability(RF_FF_1rep,binary_fv)\n",
        "        # obj4 = fitness_functional_probability(RF_RF_1rep,binary_fv)\n",
        "        to_ret = [obj1,obj2,obj3]\n",
        "        logger = logging.getLogger()\n",
        "        logger.info(str(fv)+\",\"+str(to_ret))\n",
        "        return to_ret\n",
        "\n",
        "\n",
        "def run(i,population ,archive ,second_archive, gens):\n",
        "    env=mtc_wrapped\n",
        "    d=500\n",
        "    size = len(population)\n",
        "    lb = [0, 0, 0]\n",
        "    ub = [100000,1000000,100000]\n",
        "\n",
        "    parameters = [model,d,unique1]\n",
        "    threshold_criteria = [-180, 0.04, 0.05]\n",
        "\n",
        "\n",
        "    no_of_Objectives = 3;\n",
        "\n",
        "    now = datetime.now()\n",
        "    global logger\n",
        "    logger = logging.getLogger()\n",
        "    log_file = '/content/drive/MyDrive/log/STARLA' + str(i) + '_V2' + str(now) + '.log'\n",
        "    logging.basicConfig(filename=log_file,\n",
        "                        format='%(asctime)s %(message)s')\n",
        "\n",
        "    logger.setLevel(logging.WARNING)\n",
        "\n",
        "    archive = minimize(MountainCar_caseStudy()._evaluate, population, lb, ub,\n",
        "                       no_of_Objectives, threshold_criteria, 7200, \n",
        "                       logger,archive,size,d,env , parameters, second_archive,gens)\n",
        "    logger.info(\"Iteration completed\")\n",
        "    logger.info(\"mu\"+str(MUTATION_NUMBER))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0YnEnjnGJ22"
      },
      "source": [
        "###analyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t8byJp3TGJ22"
      },
      "outputs": [],
      "source": [
        "def analyze_result(result):\n",
        "  '''\n",
        "  this function is to aggrigate the differences of the results \n",
        "  :param `result`: this is the output of the re-execution-improved function\n",
        "  :return ``:\n",
        "  '''\n",
        "  total_dif =0\n",
        "  # store_diff=[]\n",
        "  for i in range(len(result)):\n",
        "    dif = abs(result[i][1][0] - result[i][1][1])\n",
        "    # store_diff.append([i,dif])\n",
        "    total_dif += dif\n",
        "  return total_dif #, store_diff\n",
        "\n",
        "\n",
        "def get_objective_distribution_and_set_candidate_objectives(population,model,d,\n",
        "                                                            unique1,RF_FF_1rep,\n",
        "                                                            RF_RF_1rep):\n",
        "  fit1_list =[]\n",
        "  fit2_list =[]\n",
        "  fit3_list =[]\n",
        "  fit4_list =[]\n",
        "  for i in range(len(population)):\n",
        "    ind_data = population[i].get_candidate_values()\n",
        "    fit1 = fitness_reward(ind_data)\n",
        "    fit2 = fitness_confidence(ind_data,model,'m')\n",
        "    binary_fv = translator(ind_data,model,d,unique1)\n",
        "    fit3 = fitness_functional_probability(RF_FF_1rep,binary_fv)\n",
        "    fit4 = fitness_reward_probability(RF_RF_1rep,binary_fv)\n",
        "    obj = [fit1,fit2,fit3,fit4]\n",
        "    population[i].set_objective_values(obj)\n",
        "    fit1_list.append(fit1)\n",
        "    fit2_list.append(fit2)\n",
        "    fit3_list.append(fit3)\n",
        "    fit4_list.append(fit4)\n",
        "  return   fit1_list, fit2_list, fit3_list, fit4_list \n",
        "\n",
        "def get_3objective_distribution_and_set_candidate_objectives(population,model,d,\n",
        "                                                            unique1,RF_FF_1rep):\n",
        "  fit1_list =[]\n",
        "  fit2_list =[]\n",
        "  fit3_list =[]\n",
        "  for i in range(len(population)):\n",
        "    ind_data = population[i].get_candidate_values()\n",
        "    fit1 = fitness_reward_general(ind_data)\n",
        "    fit2 = fitness_confidence_general(ind_data,model,'m')\n",
        "    binary_fv = translator(ind_data,model,d,unique1)\n",
        "    fit3 = fitness_functional_probability(RF_FF_1rep,binary_fv)\n",
        "    obj = [fit1,fit2,fit3]\n",
        "    population[i].set_objective_values(obj)\n",
        "    fit1_list.append(fit1)\n",
        "    fit2_list.append(fit2)\n",
        "    fit3_list.append(fit3)\n",
        "  return   fit1_list, fit2_list, fit3_list \n",
        "\n",
        "def get_objective_distribution(population,model,d,unique1,RF_FF_1rep,RF_RF_1rep):\n",
        "  fit1_list =[]\n",
        "  fit2_list =[]\n",
        "  fit3_list =[]\n",
        "  fit4_list =[]\n",
        "  for i in range(len(population)):\n",
        "    ind_data = population[i].get_candidate_values()\n",
        "    fit1 = fitness_reward(ind_data)\n",
        "    fit2 = fitness_confidence(ind_data,model,'m')\n",
        "    binary_fv = translator(ind_data,model,d,unique1)\n",
        "    fit3 = fitness_functional_probability(RF_FF_1rep,binary_fv)\n",
        "    fit4 = fitness_reward_probability(RF_RF_1rep,binary_fv)\n",
        "    # obj = [fit1,fit2,fit3,fit4]\n",
        "    # population[i].set_objective_values(obj)\n",
        "    fit1_list.append(fit1)\n",
        "    fit2_list.append(fit2)\n",
        "    fit3_list.append(fit3)\n",
        "    fit4_list.append(fit4)\n",
        "  return   fit1_list, fit2_list, fit3_list, fit4_list \n",
        "\n",
        "\n",
        "def was_in_initial_population(solution, population,no_of_Objectives):\n",
        "  flag = False\n",
        "  for individuals_ in population:\n",
        "    if individuals_.get_objective_values() == solution.get_objective_values():\n",
        "      flag = True\n",
        "  if not flag:\n",
        "    return solution\n",
        "  if flag:\n",
        "    return 0\n",
        "\n",
        "def analyze_set_differences(differences_set):\n",
        "  '''\n",
        "  input is a set of differences \n",
        "  '''\n",
        "  analyzed_results=[]\n",
        "  for item in differences_set:\n",
        "    res = [len(item[0]),analyze_result(item[0]), item[1], len(item[0])/item[1]]\n",
        "    analyzed_results.append(res)\n",
        "  return analyzed_results\n",
        "\n",
        "def extract_differences(solution_set):\n",
        "  '''\n",
        "  input is a set of solutions like archive or second_archive \n",
        "  the output a list ([list of differences as a result of re-execution],reward)\n",
        "  '''\n",
        "  differences = []\n",
        "  for dastan in solution_set:\n",
        "    reward = dastan.get_objective_values()[0]\n",
        "    differences.append([re_execution_improved_v2(model,env,dastan),reward])\n",
        "  return differences\n",
        "  \n",
        "def get_results_distribution(results):\n",
        "  num_of_diff=[]\n",
        "  diff_confi = []\n",
        "  diff_ration = []\n",
        "  for item in results:\n",
        "    num_of_diff.append(item[0])\n",
        "    diff_confi.append(item[1])\n",
        "    diff_ration.append(item[3])\n",
        "  return num_of_diff, diff_confi, diff_ration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Swyk5eb2GJ23"
      },
      "outputs": [],
      "source": [
        "def random_test_1(model, env, Num):\n",
        "  obs=env.reset()\n",
        "  counter = 1\n",
        "  episode_reward = 0.0\n",
        "  for i in range(Num):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    episode_reward += reward\n",
        "    if done:\n",
        "      counter += 1\n",
        "      end = i\n",
        "      print(\"Reward:\", episode_reward, \"final state\", info['mem'][-2][0])\n",
        "      episode_reward = 0.0\n",
        "      obs = env.reset()\n",
        "  iter = deepcopy(counter)\n",
        "  u=1\n",
        "  while iter>1:\n",
        "    if info['mem'][-u][0]=='done':\n",
        "      lastpoint = -u\n",
        "      iter -= 1\n",
        "    u+=1\n",
        "  fin =Num - end\n",
        "  start = -Num -counter\n",
        "  randomtest = info['mem'][lastpoint:-fin]\n",
        "  ran_state = info['state'][(-counter+1):-1]\n",
        "  return randomtest , ran_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxg3ZMXfGJ23"
      },
      "source": [
        "##Model and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "3b_J8cYoGJ23",
        "outputId": "31ddeb2d-ed46-46a1-fee4-0b12253b337d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\deepq\\build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
          ]
        }
      ],
      "source": [
        "#Address of the trained RL model \n",
        "Drive_model  =\"c:/Users/Student/Desktop/Data/dqn-4-1-6-89946.zip\"\n",
        "mtc = gym.make('MountainCar-v0')\n",
        "mtc_wrapped = StoreAndTerminateWrapper(mtc)\n",
        "model = DQN('MlpPolicy',env=mtc_wrapped, verbose=1)\n",
        "model = model.load(Drive_model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dIF0d6-Eav-"
      },
      "source": [
        "#mountaincar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "juOCrE3umOBX"
      },
      "outputs": [],
      "source": [
        "def random_test_2(model, env, Num):\n",
        "  # start= len(info['mem'])\n",
        "  obs=env.reset()\n",
        "  counter = 1\n",
        "  episode_reward = 0.0\n",
        "  for i in range(Num):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    episode_reward += reward\n",
        "    if done:\n",
        "      counter += 1\n",
        "      end = i\n",
        "      episode_reward = 0.0\n",
        "      obs = env.reset()\n",
        "  iter = deepcopy(counter)\n",
        "  u=1\n",
        "  while iter>1:\n",
        "    if env.info['mem'][-u][0]=='done':\n",
        "      lastpoint = -u\n",
        "      iter -= 1\n",
        "    u+=1\n",
        "  fin =Num - end\n",
        "  start = -Num -counter\n",
        "  randomtest = env.info['mem'][lastpoint:-fin]\n",
        "  ran_state = env.info['state'][(-counter+1):-1]\n",
        "  return randomtest , ran_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NNbWmHaKmsAF"
      },
      "outputs": [],
      "source": [
        "def is_functional_fault(episode):\n",
        "  epsilon = 0.1\n",
        "  env = mtc_wrapped\n",
        "  reward = episode[-1][1]\n",
        "  last_state = episode[-2][0][0]\n",
        "  if last_state<(env.low[0]+epsilon) and reward == -200:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "\n",
        "def is_reward_fault(episode):\n",
        "  RF_threshold = -180\n",
        "  reward = episode[-1][1]\n",
        "  # print(len(episode))\n",
        "  if reward<RF_threshold and len(episode)>200:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def is_functional_fault_last_state(last_step,done_step):\n",
        "  epsilon = 0.1\n",
        "  env = mtc_wrapped\n",
        "  assert done_step[0]=='done', \"Wrong input!\"\n",
        "  reward = done_step[1]\n",
        "  last_state = last_step[0][0]\n",
        "  if last_state<(env.low[0]+epsilon) and reward == -200:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "\n",
        "def is_reward_fault_last_state(last_step,done_step):\n",
        "  RF_threshold = -180\n",
        "  assert done_step[0]=='done', \"Wrong input!\"\n",
        "  reward = done_step[1]\n",
        "  last_state = last_step[0][0]\n",
        "  # print(len(episode))\n",
        "  if reward<RF_threshold and not is_functional_fault_last_state(last_step,done_step):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj-y9BZajxhi"
      },
      "source": [
        "##wrapper with termination "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rP6klu4IjxPI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class StoreAndTerminateWrapper(gym.Wrapper):\n",
        "  ''''\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  :param max_steps: (int) Max number of steps per episode\n",
        "  '''\n",
        "  def __init__(self, env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(StoreAndTerminateWrapper, self).__init__(env)\n",
        "    self.max_steps = 200\n",
        "    # Counter of steps per episode\n",
        "    self.current_step = 0\n",
        "    self.mem = []\n",
        "    self.TotalReward = 0.0 \n",
        "    self.env = env\n",
        "    self.first_state = 0\n",
        "    self.first_obs = 0\n",
        "    self.prev_obs = 0 \n",
        "    self.states_list = []\n",
        "    self.info = {}\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    # Reset the counter\n",
        "    self.current_step = 0\n",
        "    obs =self.env.reset()\n",
        "    self.TotalReward = 0.0\n",
        "    self.first_obs = obs\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    In this function we store the initial state as well as the memory of the agent\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    if self.current_step == 0: #store initial state\n",
        "      self.prev_obs = self.first_obs\n",
        "      self.first_state = deepcopy(self.env)\n",
        "      self.states_list.append(self.first_state)\n",
        "    # print(\"t\",self.env.state[0],\"reward\",self.TotalReward)\n",
        "    self.current_step += 1\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    self.TotalReward += reward\n",
        "    self.mem.append(tuple((self.prev_obs,action)))\n",
        "    self.prev_obs = obs\n",
        "    if self.current_step >= self.max_steps:\n",
        "      done = True\n",
        "      # Update the info dict to signal that the limit was exceeded\n",
        "    if obs[0]<=-1.2:\n",
        "      done = True\n",
        "      reward = -201 - self.TotalReward\n",
        "      self.TotalReward =-200\n",
        "      # print(\"fff\",reward)\n",
        "    if done:\n",
        "      self.mem.append(tuple(('done',self.TotalReward)))\n",
        "    self.info['mem'] = self.mem\n",
        "    self.info['state'] = self.states_list\n",
        "    # self.mem.append(tuple(obs,action))\n",
        "    return obs, reward, done, info\n",
        "\n",
        "  def set_state(self, state):\n",
        "    \"\"\"\n",
        "    :param state: initial state of the episode\n",
        "    :return: environment is updated and observations is returned\n",
        "    \"\"\"\n",
        "    self.env = deepcopy(state)\n",
        "    obs = np.array(list(self.env.unwrapped.state))\n",
        "    self.current_step = 0\n",
        "    self.TotalReward = 0.0\n",
        "    self.first_obs = obs\n",
        "    return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PweRzX9Ap0du"
      },
      "source": [
        "#Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_951PY1p0du",
        "outputId": "3aadacec-b2b1-49ae-97c9-a522380243c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\ipykernel_launcher.py:479: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "d= 500 \n",
            " abs 93 \n",
            " abs 93\n"
          ]
        }
      ],
      "source": [
        "ee,qq=random_test_2(model,mtc_wrapped,100_000)\n",
        "test, teststate = fix_testing(ee,qq,mtc_wrapped)\n",
        "\n",
        "\n",
        "\n",
        "Read_from_data = True\n",
        "d=500\n",
        "\n",
        "\n",
        "if Read_from_data:\n",
        "    with open(f'c:/Users/Student/Desktop/Data/Abstraction/Abstraction_data_sampled_200_{d}.pickle', 'rb') as file2:\n",
        "        unique1 = pickle.load(file2)\n",
        "    print(\"\\nd=\",d,'\\n abs',len(unique1),'\\n abs',len(set(unique1))) # check the numbers to see if we have any duplicated abstract state\n",
        "    uni1 = np.array(unique1) #array for late use\n",
        "if not Read_from_data:\n",
        "  unique1,uni1 = Abstract_classes(test,d,model)\n",
        "\n",
        "count =0\n",
        "X_RT =[]\n",
        "Y_RT = []\n",
        "# functional_fault = False\n",
        "# reward_fault_threshold = -180\n",
        "for episode in test:\n",
        "  record = np.zeros(len(uni1))\n",
        "  for state, action in episode:\n",
        "    ab = abstract_state_general(model,state,d)\n",
        "    if ab == 'end':\n",
        "      if is_functional_fault(episode):\n",
        "        Y_RT.append(1)\n",
        "      else:\n",
        "        Y_RT.append(0)\n",
        "      continue\n",
        "    if ab in unique1:\n",
        "      ind = unique1.index(ab)\n",
        "      record[ind] = 1\n",
        "    else:\n",
        "      count+=1\n",
        "  X_RT.append(record)\n",
        "\n",
        "\n",
        "\n",
        "fix_data = pd.DataFrame(X_RT, columns=[ f'abs{i}' for i in range(len(X_RT[0]))]) \n",
        "fix_data['Decision'] = Y_RT\n",
        "\n",
        "data_X_0_fix=fix_data[fix_data.Decision==0]\n",
        "data_X_1_fix=fix_data[fix_data.Decision==1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pandas.core.frame import DataFrame\n",
        "def KfolD_report4(X , Y , K_split):\n",
        "  kf = KFold(n_splits=K_split)\n",
        "  Result_Train = 0\n",
        "  R_test = 0\n",
        "  Accu_train = 0\n",
        "  Accu_test = 0\n",
        "  list_test_accuracy = []\n",
        "\n",
        "  list_accuracy = []\n",
        "  list_f1 = []\n",
        "  list_wa_pre = []\n",
        "  list_wa_rec = []\n",
        "  CLF_Report=[]\n",
        "  for trainIndex, testIndex in kf.split(X):\n",
        "    trainX, X_test = X.values[trainIndex], X.values[testIndex]\n",
        "    trainY, Y_test = Y.values[trainIndex], Y.values[testIndex]\n",
        "    ML_model = tree.DecisionTreeClassifier(class_weight='balanced',min_impurity_decrease=0.02,min_samples_leaf=10,min_samples_split=15)\n",
        "    # ML_model = tree.DecisionTreeClassifier(class_weight='balanced')\n",
        "    ML_model.fit(trainX, trainY.ravel())\n",
        "\n",
        "\n",
        "    ypred_test = ML_model.predict(X_test)\n",
        "\n",
        "    wa_f1 = f1_score(Y_test, ypred_test, average='weighted')\n",
        "    acc = accuracy_score(Y_test, ypred_test)\n",
        "    wa_pre = metrics.precision_score(Y_test, ypred_test, average='weighted')\n",
        "    wa_rec = metrics.recall_score(Y_test, ypred_test, average='weighted')\n",
        "\n",
        "    list_accuracy.append(acc)\n",
        "    list_f1.append(wa_f1)\n",
        "    list_wa_pre.append(wa_pre)\n",
        "    list_wa_rec.append(wa_rec)\n",
        "    CLF_Report.append(classification_report(Y_test,ypred_test,output_dict=True,digits=4))\n",
        "    del ML_model\n",
        "  return   [list_accuracy, list_f1, list_wa_pre, list_wa_rec,CLF_Report]\n",
        "\n",
        "def change_threshold(new_threshold,results):\n",
        "  return_set=[]\n",
        "  for re_exe in results:\n",
        "    episode = re_exe[3]\n",
        "    ff_prob = episode.get_objective_values()[2]\n",
        "    if ff_prob<=new_threshold:\n",
        "      return_set.append(re_exe)\n",
        "  return return_set\n",
        "\n",
        "def magnifier(metric_list):\n",
        "  final_list= [c*100 for c in metric_list]\n",
        "  return final_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def report_ML_model(model2,x_train, y_train,x_test, y_test):\n",
        "  print(\"********************** reporting the result of the model **************************\")\n",
        "  print('The score for train data is {0}'.format(model2.score(x_train,y_train)))\n",
        "  print('The score for test data is {0}'.format(model2.score(x_test,y_test)))\n",
        "\n",
        "\n",
        "  predictions_train = model2.predict(x_train)\n",
        "  predictions_test = model2.predict(x_test)\n",
        "\n",
        "  print(\"\\n\\n--------------------------------------recall---------------------------------\")\n",
        "\n",
        "  print('the test recall for the class yes is {0}'.format(metrics.recall_score(y_test,predictions_test, pos_label=1)))\n",
        "  print('the test recall for the class no is {0}'.format(metrics.recall_score(y_test,predictions_test, pos_label=0)))\n",
        "\n",
        "  print('the training recall for the class yes is {0}'.format(metrics.recall_score(y_train,predictions_train, pos_label=1)))\n",
        "  print('the training recall for the class no is {0}'.format(metrics.recall_score(y_train,predictions_train, pos_label=0)))\n",
        "\n",
        "\n",
        "  print(\"\\n\\n--------------------------------------precision------------------------------\")\n",
        "\n",
        "\n",
        "  print('the test precision for the class yes is {0}'.format(metrics.precision_score(y_test,predictions_test, pos_label=1)))\n",
        "  print('the test precision for the class no is {0}'.format(metrics.precision_score(y_test,predictions_test, pos_label=0)))\n",
        "\n",
        "  print('the training precision for the class yes is {0}'.format(metrics.precision_score(y_train,predictions_train, pos_label=1)))\n",
        "  print('the training precision for the class no is {0}'.format(metrics.precision_score(y_train,predictions_train, pos_label=0)))\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "  print(classification_report(y_test, predictions_test, target_names=['NO ','yes']))\n",
        "\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, predictions_test).ravel()\n",
        "  specificity = tn / (tn+fp)\n",
        "  print(\"\\n\\nspecifity :\",specificity)\n",
        "  print(\"\\n\\n--------------------------------------confusion----------------------------\")\n",
        "  CM = metrics.confusion_matrix(y_test, predictions_test)\n",
        "  print(\"The confusion Matrix:\")\n",
        "  print(CM)\n",
        "  print('the accuracy score in {0}\\n\\n'.format(accuracy_score(y_test, predictions_test)))\n",
        "  print(\"********************** plotting the confusion matrix & ROC curve **************************\")\n",
        "  plot_confusion_matrix(model2, x_test, y_test)\n",
        "  metrics.plot_roc_curve(model2, x_test, y_test) \n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4f8HxkGMjOV",
        "outputId": "baefe94a-9639-4291-997e-785efbc53ba4"
      },
      "outputs": [],
      "source": [
        "def change_threshold_in_similarity_data(new_threshold,results):\n",
        "  return_set=[]\n",
        "  for re_exe in results:\n",
        "    episode = re_exe[4]\n",
        "    ff_prob = episode.get_objective_values()[2]\n",
        "    if ff_prob<=new_threshold:\n",
        "      return_set.append(re_exe)\n",
        "  return return_set\n",
        "def change_threshold_General(new_threshold,results):\n",
        "  return_set=[]\n",
        "  for re_exe in results:\n",
        "    episode = re_exe[-1]\n",
        "    ff_prob = episode.get_objective_values()[2]\n",
        "    if ff_prob<=new_threshold:\n",
        "      return_set.append(re_exe)\n",
        "  return return_set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mbKdCpAp0dw"
      },
      "source": [
        "#RQ3 Functional fault"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "YP7q74sIp0dw",
        "outputId": "c4ce16a3-7c1d-4bae-894f-597b3f59eca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run0_0.pickle.pickle\n",
            "len second arch1: 338\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run0_1.pickle.pickle\n",
            "len second arch1: 511\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run0_2.pickle.pickle\n",
            "len second arch1: 412\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run0_3.pickle.pickle\n",
            "len second arch1: 545\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run0_4.pickle.pickle\n",
            "len second arch1: 502\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run0_5.pickle.pickle\n",
            "len second arch1: 476\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run0_6.pickle.pickle\n",
            "len second arch1: 360\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run0_7.pickle.pickle\n",
            "len second arch1: 461\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run0_8.pickle.pickle\n",
            "len second arch1: 523\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run0_9.pickle.pickle\n",
            "len second arch1: 545\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run1_0.pickle.pickle\n",
            "len second arch1: 469\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run1_1.pickle.pickle\n",
            "len second arch1: 427\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run1_2.pickle.pickle\n",
            "len second arch1: 464\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run1_3.pickle.pickle\n",
            "len second arch1: 649\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run1_4.pickle.pickle\n",
            "len second arch1: 412\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run1_5.pickle.pickle\n",
            "len second arch1: 421\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run1_6.pickle.pickle\n",
            "len second arch1: 450\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run1_7.pickle.pickle\n",
            "len second arch1: 506\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run1_8.pickle.pickle\n",
            "len second arch1: 491\n",
            "c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim/re_executedDec07_generations_population1000lastfull_run1_9.pickle.pickle\n",
            "len second arch1: 396\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pff = 0.5\n",
        "import os\n",
        "path_to_re_exe = 'c:/Users/Student/Desktop/Data/NewResults1207/Exe_Sim'\n",
        "items = os.listdir(path_to_re_exe)\n",
        "\n",
        "list_n_ff=[]\n",
        "list_n_rf=[]\n",
        "tree_test_report=[]\n",
        "RF_test_report=[]\n",
        "tree_test_report_ff=[]\n",
        "RF_test_report_ff=[]\n",
        "\n",
        "finalx=[]\n",
        "finaly= []\n",
        "for item in items[:]:\n",
        "  print(f'{path_to_re_exe}/{item}')\n",
        "  with open(f'{path_to_re_exe}/{item}', 'rb') as file2:\n",
        "      data = pickle.load(file2)\n",
        "\n",
        "  final_consistent_ff =[] \n",
        "\n",
        "  new_data = change_threshold_General(pff,data)\n",
        "  for result in new_data:\n",
        "    inconsistent, div , ff,states, episode_data = result\n",
        "    if ff:\n",
        "      if not inconsistent:\n",
        "        final_consistent_ff.append(episode_data)\n",
        "\n",
        "  print(\"len second arch1:\",len(final_consistent_ff))\n",
        "  list_n_ff.append(len(final_consistent_ff))\n",
        "\n",
        "\n",
        "\n",
        "  X_RU_6 =[]\n",
        "  data1_y_b= [] \n",
        "  Y_RU_6 = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for candid in final_consistent_ff:\n",
        "    episode = candid.get_candidate_values()\n",
        "    record = np.zeros(len(uni1))\n",
        "    for state, action in episode:\n",
        "      ab = abstract_state_general(model,state,d)\n",
        "      if ab == 'end':\n",
        "        if is_functional_fault(episode):\n",
        "          Y_RU_6.append(1)\n",
        "        else:\n",
        "          Y_RU_6.append(0)\n",
        "        continue\n",
        "      if ab in unique1:\n",
        "        ind = unique1.index(ab)\n",
        "        record[ind] = 1\n",
        "    X_RU_6.append(record)\n",
        "\n",
        "\n",
        "########################################################--------------------------------------------------------------------------------\n",
        "##                  functional Fault                  ##\n",
        "########################################################--------------------------------------------------------------------------------\n",
        "\n",
        "  RU_7_data = pd.DataFrame(X_RU_6, columns=[ f'abs{i}' for i in range(len(X_RU_6[0]))]) \n",
        "  RU_7_data['Decision'] = Y_RU_6\n",
        "\n",
        "  data_X_ff_label1=RU_7_data[RU_7_data.Decision==1]\n",
        "\n",
        "  data_X_0_fix_balance_FF = data_X_0_fix[:len(data_X_ff_label1)]\n",
        "  data_new_ff = pd.concat([data_X_ff_label1,data_X_0_fix_balance_FF])\n",
        "\n",
        "  RU_shuffle2 = shuffle(data_new_ff)\n",
        "  dxf = RU_shuffle2.iloc[:,:-1]\n",
        "  dyf = RU_shuffle2.iloc[:,-1:]\n",
        "  finalx.append(dxf)\n",
        "  finaly.append(dyf)\n",
        "########################################################--------------------------------------------------------------------------------\n",
        "  res2_ff =KfolD_report4(dxf,dyf,10)\n",
        "  tree_test_report_ff.append(res2_ff)\n",
        "########################################################--------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def KfolD_report4(X , Y , K_split):\n",
        "  kf = KFold(n_splits=K_split)\n",
        "  Result_Train = 0\n",
        "  R_test = 0\n",
        "  Accu_train = 0\n",
        "  Accu_test = 0\n",
        "  list_test_accuracy = []\n",
        "\n",
        "  list_accuracy = []\n",
        "  list_f1 = []\n",
        "  list_wa_pre = []\n",
        "  list_wa_rec = []\n",
        "  CLF_Report=[]\n",
        "  for trainIndex, testIndex in kf.split(X):\n",
        "    trainX, X_test = X.values[trainIndex], X.values[testIndex]\n",
        "    trainY, Y_test = Y.values[trainIndex], Y.values[testIndex]\n",
        "    ML_model = tree.DecisionTreeClassifier(class_weight='balanced',min_impurity_decrease=0.055,min_samples_leaf=5,min_samples_split=10)\n",
        "    # ML_model = tree.DecisionTreeClassifier(class_weight='balanced')\n",
        "    ML_model.fit(trainX, trainY.ravel())\n",
        "\n",
        "\n",
        "    ypred_test = ML_model.predict(X_test)\n",
        "\n",
        "    wa_f1 = f1_score(Y_test, ypred_test, average='weighted')\n",
        "    acc = accuracy_score(Y_test, ypred_test)\n",
        "    wa_pre = metrics.precision_score(Y_test, ypred_test, average='weighted')\n",
        "    wa_rec = metrics.recall_score(Y_test, ypred_test, average='weighted')\n",
        "\n",
        "    list_accuracy.append(acc)\n",
        "    list_f1.append(wa_f1)\n",
        "    list_wa_pre.append(wa_pre)\n",
        "    list_wa_rec.append(wa_rec)\n",
        "    CLF_Report.append(classification_report(Y_test,ypred_test,output_dict=True,digits=4))\n",
        "    del ML_model\n",
        "  return   [list_accuracy, list_f1, list_wa_pre, list_wa_rec,CLF_Report]\n",
        "\n",
        "tree_test_report_ff=[]\n",
        "\n",
        "for i in range(20):\n",
        "  res2_ff=KfolD_report4(finalx[i],finaly[i],10)\n",
        "  tree_test_report_ff.append(res2_ff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "import matplotlib.ticker as mtick\n",
        "\n",
        "f1_list =[]\n",
        "pre_list =[]\n",
        "rec_list =[]\n",
        "\n",
        "for result in tree_test_report_ff:\n",
        "  report_dict = result[4]\n",
        "  f1=0\n",
        "  pre=0\n",
        "  rec=0\n",
        "  for report in report_dict:\n",
        "    f1 += report[\"1\"][\"f1-score\"]\n",
        "    pre += report[\"1\"][\"precision\"]\n",
        "    rec += report[\"1\"][\"recall\"]\n",
        "  f1_list.append(f1/len(report_dict))\n",
        "  pre_list.append(pre/len(report_dict))\n",
        "  rec_list.append(rec/len(report_dict))\n",
        "data_r = [f1_list,pre_list,rec_list]\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "bp = ax.boxplot(data_r)\n",
        "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=None, symbol='%', is_latex=False))\n",
        "plt.xticks([1, 2, 3], [\"F1-score\",\"Precision\",\"Recall\"])\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "p4NBDxnDGJ2y",
        "MqA8M1BJGJ2z",
        "0uMfthk5GJ2z",
        "3kYTYP6MGJ20",
        "0dIF0d6-Eav-",
        "551EeqsMp0dy",
        "-goCyUPatLZa"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.8 ('venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "704142a53cc302aff65c9d37244a965c0815bdc7e36e6b2c7e9e17e0904d256a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
